{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b698ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoIDs</th>\n",
       "      <th>joined_data</th>\n",
       "      <th>labels_ext_annotation</th>\n",
       "      <th>labels_selfassessment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10]</td>\n",
       "      <td>[[9.736634779916856e-06, 5.787642152785703e-06...</td>\n",
       "      <td>[[1.0, 0.05776438529430914, -0.043448843503250...</td>\n",
       "      <td>[[2.938568, 5.0, 8.235496000000001, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[13]</td>\n",
       "      <td>[[8.566842327966853e-06, 6.519705733341464e-06...</td>\n",
       "      <td>[[1.0, 0.03924276666387655, -0.078229774094844...</td>\n",
       "      <td>[[4.877136, 6.542664, 4.849832, 9.0, 9.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[138]</td>\n",
       "      <td>[[6.941397026022523e-06, 5.6470783809481336e-0...</td>\n",
       "      <td>[[1.0, -0.09449048110636794, -0.07563033025492...</td>\n",
       "      <td>[[5.0, 3.2389040000000002, 2.774744, 1.327648,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[18]</td>\n",
       "      <td>[[6.2782001878572814e-06, 6.331029538677923e-0...</td>\n",
       "      <td>[[1.0, -0.1435269663234843, -0.073054137254354...</td>\n",
       "      <td>[[2.965872, 8.590440000000001, 8.098976, 8.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19]</td>\n",
       "      <td>[[8.362128794421077e-06, 5.747076729022019e-06...</td>\n",
       "      <td>[[1.0, -0.04503269068013355, -0.09591203742455...</td>\n",
       "      <td>[[6.460752, 1.0, 2.856656, 1.0, 9.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VideoIDs                                        joined_data  \\\n",
       "0     [10]  [[9.736634779916856e-06, 5.787642152785703e-06...   \n",
       "1     [13]  [[8.566842327966853e-06, 6.519705733341464e-06...   \n",
       "2    [138]  [[6.941397026022523e-06, 5.6470783809481336e-0...   \n",
       "3     [18]  [[6.2782001878572814e-06, 6.331029538677923e-0...   \n",
       "4     [19]  [[8.362128794421077e-06, 5.747076729022019e-06...   \n",
       "\n",
       "                               labels_ext_annotation  \\\n",
       "0  [[1.0, 0.05776438529430914, -0.043448843503250...   \n",
       "1  [[1.0, 0.03924276666387655, -0.078229774094844...   \n",
       "2  [[1.0, -0.09449048110636794, -0.07563033025492...   \n",
       "3  [[1.0, -0.1435269663234843, -0.073054137254354...   \n",
       "4  [[1.0, -0.04503269068013355, -0.09591203742455...   \n",
       "\n",
       "                               labels_selfassessment  \n",
       "0  [[2.938568, 5.0, 8.235496000000001, 1.0, 1.0, ...  \n",
       "1  [[4.877136, 6.542664, 4.849832, 9.0, 9.0, 0.0,...  \n",
       "2  [[5.0, 3.2389040000000002, 2.774744, 1.327648,...  \n",
       "3  [[2.965872, 8.590440000000001, 8.098976, 8.208...  \n",
       "4  [[6.460752, 1.0, 2.856656, 1.0, 9.0, 0.0, 1.0,...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_paths = [\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P01.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P02.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P03.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P04.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P05.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P06.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P07.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P08.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P09.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P10.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P11.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P12.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P13.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P14.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P15.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P16.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P17.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P18.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P19.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P20.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P21.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P22.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P23.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P24.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P25.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P26.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P27.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P28.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P29.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P30.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P31.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P32.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P33.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P34.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P35.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P36.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P37.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P38.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P39.mat\",\n",
    "    \"C:/Users/moham/Resarch project/data_preprocessed/data_preprocessed/Data_Preprocessed_P40.mat\"\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "\n",
    "    \n",
    "    reshaped_data = {}\n",
    "\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, (list, np.ndarray)):\n",
    "            if isinstance(value, list):\n",
    "            \n",
    "                value = np.array(value)\n",
    "            if len(value.shape) > 1:\n",
    "\n",
    "                reshaped_data[key] = value.flatten()\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(reshaped_data)\n",
    "\n",
    "    dataframes.append(df)\n",
    "\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0251a30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [[9.736634779916856e-06, 5.787642152785703e-06...\n",
      "1      [[8.566842327966853e-06, 6.519705733341464e-06...\n",
      "2      [[6.941397026022523e-06, 5.6470783809481336e-0...\n",
      "3      [[6.2782001878572814e-06, 6.331029538677923e-0...\n",
      "4      [[8.362128794421077e-06, 5.747076729022019e-06...\n",
      "                             ...                        \n",
      "795    [[1.0049837293000902e-05, 5.651795288717108e-0...\n",
      "796    [[7.878174299048666e-06, 6.108391668441773e-06...\n",
      "797    [[4.726338572289476e-06, 3.11504389088537e-06,...\n",
      "798    [[1.1901693991916231e-05, 4.941429406526385e-0...\n",
      "799    [[2.7348613597247116e-06, 6.895171341535172e-0...\n",
      "Name: GSR, Length: 800, dtype: object\n",
      "                                                 first  second\n",
      "0    [2.938568, 5.0, 8.235496000000001, 1.0, 1.0, 1...       0\n",
      "1    [4.877136, 6.542664, 4.849832, 9.0, 9.0, 0.0, ...       0\n",
      "2    [5.0, 3.2389040000000002, 2.774744, 1.327648, ...       0\n",
      "3    [2.965872, 8.590440000000001, 8.098976, 8.2081...       0\n",
      "4    [6.460752, 1.0, 2.856656, 1.0, 9.0, 0.0, 1.0, ...       0\n",
      "..                                                 ...     ...\n",
      "795  [6.450848000000001, 6.098304, 4.17288, 9.0, 8....       0\n",
      "796  [6.04, 3.0, 4.04, 2.76, 1.08, 1.0, 0.0, 0.0, 1...       0\n",
      "797  [3.24, 5.96, 6.92, 6.52, 1.0, 1.0, 0.0, 1.0, 0...       0\n",
      "798  [6.28, 6.68, 6.04, 9.16, 9.16, 1.0, 0.0, 1.0, ...       0\n",
      "799  [6.92, 7.32, 2.84, 8.12, 1.0, 0.0, 0.0, 1.0, 1...       0\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "      arousal   valence\n",
      "0    2.938568  5.000000\n",
      "1    4.877136  6.542664\n",
      "2    5.000000  3.238904\n",
      "3    2.965872  8.590440\n",
      "4    6.460752  1.000000\n",
      "..        ...       ...\n",
      "795  6.450848  6.098304\n",
      "796  6.040000  3.000000\n",
      "797  3.240000  5.960000\n",
      "798  6.280000  6.680000\n",
      "799  6.920000  7.320000\n",
      "\n",
      "[800 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arousal</th>\n",
       "      <th>valence</th>\n",
       "      <th>GSR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.938568</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>[[9.736634779916856e-06, 5.787642152785703e-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.877136</td>\n",
       "      <td>6.542664</td>\n",
       "      <td>[[8.566842327966853e-06, 6.519705733341464e-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.238904</td>\n",
       "      <td>[[6.941397026022523e-06, 5.6470783809481336e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.965872</td>\n",
       "      <td>8.590440</td>\n",
       "      <td>[[6.2782001878572814e-06, 6.331029538677923e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.460752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[8.362128794421077e-06, 5.747076729022019e-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>6.450848</td>\n",
       "      <td>6.098304</td>\n",
       "      <td>[[1.0049837293000902e-05, 5.651795288717108e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>6.040000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>[[7.878174299048666e-06, 6.108391668441773e-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>3.240000</td>\n",
       "      <td>5.960000</td>\n",
       "      <td>[[4.726338572289476e-06, 3.11504389088537e-06,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>6.280000</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>[[1.1901693991916231e-05, 4.941429406526385e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>6.920000</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>[[2.7348613597247116e-06, 6.895171341535172e-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      arousal   valence                                                GSR\n",
       "0    2.938568  5.000000  [[9.736634779916856e-06, 5.787642152785703e-06...\n",
       "1    4.877136  6.542664  [[8.566842327966853e-06, 6.519705733341464e-06...\n",
       "2    5.000000  3.238904  [[6.941397026022523e-06, 5.6470783809481336e-0...\n",
       "3    2.965872  8.590440  [[6.2782001878572814e-06, 6.331029538677923e-0...\n",
       "4    6.460752  1.000000  [[8.362128794421077e-06, 5.747076729022019e-06...\n",
       "..        ...       ...                                                ...\n",
       "795  6.450848  6.098304  [[1.0049837293000902e-05, 5.651795288717108e-0...\n",
       "796  6.040000  3.000000  [[7.878174299048666e-06, 6.108391668441773e-06...\n",
       "797  3.240000  5.960000  [[4.726338572289476e-06, 3.11504389088537e-06,...\n",
       "798  6.280000  6.680000  [[1.1901693991916231e-05, 4.941429406526385e-0...\n",
       "799  6.920000  7.320000  [[2.7348613597247116e-06, 6.895171341535172e-0...\n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "GSR = pd.DataFrame()\n",
    "GSR['GSR'] = final_df['joined_data']\n",
    "GSR['GSR'] = GSR['GSR'].apply(lambda x: x[-20:] if isinstance(x, list) else x)\n",
    "print(GSR['GSR'])\n",
    "\n",
    "Amigos00 = pd.DataFrame()\n",
    "\n",
    "Amigos00[['first', 'second']] = final_df['labels_selfassessment'].apply(lambda x: pd.Series([x[0] if (x is not None and len(x) > 0) else 0, x[-2] if (x is not None and len(x) > 1) else 0]))\n",
    "\n",
    "\n",
    "Amigos00.fillna(0, inplace=True)\n",
    "\n",
    "print(Amigos00[['first', 'second']])\n",
    "\n",
    "Amigos0 = pd.DataFrame()\n",
    "\n",
    "Amigos0[['arousal', 'valence']] = Amigos00['first'].apply(lambda x: pd.Series([x[0] if (x is not None and len(x) > 0) else 0, x[1] if (x is not None and len(x) > 1) else 0]))\n",
    "\n",
    "print(Amigos0[['arousal', 'valence']])\n",
    "\n",
    "Amigos0['GSR'] = GSR['GSR']\n",
    "\n",
    "# Amigos0.to_excel('Amigos0.xlsx', index=False)\n",
    "\n",
    "Amigos0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4f6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      arousal   valence                                                GSR\n",
      "0    2.938568  5.000000  [[9.736634779916856e-06, 5.787642152785703e-06...\n",
      "1    4.877136  6.542664  [[8.566842327966853e-06, 6.519705733341464e-06...\n",
      "2    5.000000  3.238904  [[6.941397026022523e-06, 5.6470783809481336e-0...\n",
      "3    2.965872  8.590440  [[6.2782001878572814e-06, 6.331029538677923e-0...\n",
      "4    6.460752  1.000000  [[8.362128794421077e-06, 5.747076729022019e-06...\n",
      "..        ...       ...                                                ...\n",
      "795  6.450848  6.098304  [[1.0049837293000902e-05, 5.651795288717108e-0...\n",
      "796  6.040000  3.000000  [[7.878174299048666e-06, 6.108391668441773e-06...\n",
      "797  3.240000  5.960000  [[4.726338572289476e-06, 3.11504389088537e-06,...\n",
      "798  6.280000  6.680000  [[1.1901693991916231e-05, 4.941429406526385e-0...\n",
      "799  6.920000  7.320000  [[2.7348613597247116e-06, 6.895171341535172e-0...\n",
      "\n",
      "[800 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Amigosn = Amigos0.fillna(0)\n",
    "\n",
    "\n",
    "# Amigosn.to_excel('Amigosn.xlsx', index=False)\n",
    "\n",
    "print(Amigosn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed0e9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[9.736634779916856e-06, 5.787642152785703e-06...\n",
       "1    [[8.566842327966853e-06, 6.519705733341464e-06...\n",
       "2    [[6.941397026022523e-06, 5.6470783809481336e-0...\n",
       "3    [[6.2782001878572814e-06, 6.331029538677923e-0...\n",
       "4    [[8.362128794421077e-06, 5.747076729022019e-06...\n",
       "Name: GSR, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amigos0['GSR'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e17ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arousal</th>\n",
       "      <th>valence</th>\n",
       "      <th>GSR</th>\n",
       "      <th>emotion_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.938568</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22080.029612</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.877136</td>\n",
       "      <td>6.542664</td>\n",
       "      <td>24385.023872</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.238904</td>\n",
       "      <td>21047.319425</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.965872</td>\n",
       "      <td>8.590440</td>\n",
       "      <td>15939.235580</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.460752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17225.661715</td>\n",
       "      <td>HALV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    arousal   valence           GSR emotion_class\n",
       "0  2.938568  5.000000  22080.029612          LALV\n",
       "1  4.877136  6.542664  24385.023872          LAHV\n",
       "2  5.000000  3.238904  21047.319425          LALV\n",
       "3  2.965872  8.590440  15939.235580          LAHV\n",
       "4  6.460752  1.000000  17225.661715          HALV"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Amigosnn = pd.DataFrame()\n",
    "Amigosnn['arousal_category'] = np.where(Amigosn['arousal'] > 5, 'HA', 'LA')\n",
    "Amigosnn['valence_category'] = np.where(Amigosn['valence'] > 5, 'HV', 'LV')\n",
    "Amigosnn['combined_category'] = Amigosnn['arousal_category'] + Amigosnn['valence_category']\n",
    "\n",
    "Amigosn['emotion_class'] = Amigosnn['combined_category'] \n",
    "# columns_to_drop = ['arousal_category', 'valence_category', 'Classes']\n",
    "\n",
    "# Amigosn.drop(columns=columns_to_drop, inplace=True)\n",
    "# Assuming 'joined_data' is a column of arrays\n",
    "Amigosn['GSR'] = Amigosn['GSR'].apply(lambda x: np.mean([val for val in x if val is not None]) if isinstance(x, (list, np.ndarray)) and len(x) > 0 else None)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "\n",
    "# Amigosn.to_excel('Amigosn_with_mean_and_emotion_class.xlsx', index=False)\n",
    "Amigosn.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8e7fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column 'emotion_class' contains 'HVLA'.\n"
     ]
    }
   ],
   "source": [
    "if 'HALV' in Amigosn['emotion_class'].values:\n",
    "    print(f\"The column '{'emotion_class'}' contains 'HVLA'.\")\n",
    "else:\n",
    "    print(f\"The column '{'emotion_class'}' does not contain 'HVLA'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4db887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  valence_category\n",
       "0               LV\n",
       "1               HV\n",
       "2               LV\n",
       "3               HV\n",
       "4               LV"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amigosn_with_valence = pd.DataFrame()\n",
    "\n",
    "Amigosn_with_valence['valence_category'] = np.where(Amigosn['valence'] > 5, 'HV', 'LV')\n",
    "Amigosn_with_valence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48aa179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.24      0.24      0.24        37\n",
      "        HALV       0.29      0.80      0.42        44\n",
      "        LAHV       1.00      0.00      0.00        45\n",
      "        LALV       1.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.28       160\n",
      "   macro avg       0.63      0.26      0.17       160\n",
      "weighted avg       0.63      0.28      0.17       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "Amigosn['GSR'].fillna(0, inplace=True)  \n",
    "\n",
    "X = Amigosn[['GSR']]\n",
    "y = Amigosn['emotion_class']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f6a9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores for Each Fold:\n",
      "[0.2625 0.275  0.2625 0.25   0.325  0.3125 0.275  0.275  0.275  0.2375]\n",
      "\n",
      "Mean Accuracy: 0.2750\n",
      "Standard Deviation: 0.0250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Amigosn['GSR'].fillna(0, inplace=True)  \n",
    "X = Amigosn[['GSR']]\n",
    "y = Amigosn['emotion_class']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"Accuracy Scores for Each Fold:\")\n",
    "print(cv_scores)\n",
    "\n",
    "print(f\"\\nMean Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29040144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.23      0.46      0.31        37\n",
      "        HALV       0.31      0.61      0.41        44\n",
      "        LAHV       1.00      0.00      0.00        45\n",
      "        LALV       1.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.28       160\n",
      "   macro avg       0.64      0.27      0.18       160\n",
      "weighted avg       0.63      0.28      0.18       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "Amigosn['GSR'].fillna(0, inplace=True)  \n",
    "\n",
    "X = Amigosn[['GSR']]\n",
    "y = Amigosn['emotion_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = mlp_classifier.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83b8018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.20      0.15      0.17        20\n",
      "        HALV       0.32      0.87      0.47        23\n",
      "        LAHV       0.50      0.05      0.10        19\n",
      "        LALV       0.00      0.00      1.00        18\n",
      "\n",
      "    accuracy                           0.30        80\n",
      "   macro avg       0.26      0.27      0.43        80\n",
      "weighted avg       0.26      0.30      0.43        80\n",
      "\n",
      "Fold 2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.21      0.20      0.21        20\n",
      "        HALV       0.31      0.52      0.39        23\n",
      "        LAHV       0.32      0.37      0.34        19\n",
      "        LALV       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.29        80\n",
      "   macro avg       0.46      0.27      0.23        80\n",
      "weighted avg       0.44      0.29      0.24        80\n",
      "\n",
      "Fold 3 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.25      0.14      0.18        21\n",
      "        HALV       0.27      0.59      0.37        22\n",
      "        LAHV       0.10      0.11      0.10        19\n",
      "        LALV       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.23        80\n",
      "   macro avg       0.41      0.21      0.16        80\n",
      "weighted avg       0.39      0.23      0.17        80\n",
      "\n",
      "Fold 4 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.27      0.19      0.22        21\n",
      "        HALV       0.28      0.50      0.36        22\n",
      "        LAHV       0.23      0.32      0.27        19\n",
      "        LALV       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.26        80\n",
      "   macro avg       0.44      0.25      0.21        80\n",
      "weighted avg       0.43      0.26      0.22        80\n",
      "\n",
      "Fold 5 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.33      0.19      0.24        21\n",
      "        HALV       0.31      0.73      0.43        22\n",
      "        LAHV       0.50      0.26      0.34        19\n",
      "        LALV       0.17      0.06      0.08        18\n",
      "\n",
      "    accuracy                           0.33        80\n",
      "   macro avg       0.33      0.31      0.28        80\n",
      "weighted avg       0.33      0.33      0.28        80\n",
      "\n",
      "Fold 6 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.09      0.05      0.06        21\n",
      "        HALV       0.29      0.50      0.37        22\n",
      "        LAHV       0.44      0.37      0.40        19\n",
      "        LALV       0.20      0.17      0.18        18\n",
      "\n",
      "    accuracy                           0.28        80\n",
      "   macro avg       0.25      0.27      0.25        80\n",
      "weighted avg       0.25      0.28      0.25        80\n",
      "\n",
      "Fold 7 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.22      0.10      0.13        21\n",
      "        HALV       0.24      0.50      0.33        22\n",
      "        LAHV       0.33      0.25      0.29        20\n",
      "        LALV       0.18      0.12      0.14        17\n",
      "\n",
      "    accuracy                           0.25        80\n",
      "   macro avg       0.25      0.24      0.22        80\n",
      "weighted avg       0.25      0.25      0.23        80\n",
      "\n",
      "Fold 8 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.22      0.29      0.25        21\n",
      "        HALV       0.31      0.57      0.40        23\n",
      "        LAHV       0.00      0.00      1.00        19\n",
      "        LALV       1.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.24        80\n",
      "   macro avg       0.38      0.21      0.41        80\n",
      "weighted avg       0.36      0.24      0.42        80\n",
      "\n",
      "Fold 9 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.20      0.05      0.08        21\n",
      "        HALV       0.24      0.48      0.32        23\n",
      "        LAHV       0.26      0.32      0.29        19\n",
      "        LALV       0.43      0.18      0.25        17\n",
      "\n",
      "    accuracy                           0.26        80\n",
      "   macro avg       0.28      0.25      0.23        80\n",
      "weighted avg       0.28      0.26      0.23        80\n",
      "\n",
      "Fold 10 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.39      0.33      0.36        21\n",
      "        HALV       0.31      0.57      0.40        23\n",
      "        LAHV       0.25      0.26      0.26        19\n",
      "        LALV       1.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.31        80\n",
      "   macro avg       0.49      0.29      0.25        80\n",
      "weighted avg       0.46      0.31      0.27        80\n",
      "\n",
      "Average Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HAHV       0.32      0.31      0.31       208\n",
      "        HALV       0.32      0.57      0.41       225\n",
      "        LAHV       0.30      0.30      0.30       191\n",
      "        LALV       1.00      0.00      0.00       176\n",
      "\n",
      "    accuracy                           0.31       800\n",
      "   macro avg       0.48      0.30      0.26       800\n",
      "weighted avg       0.46      0.31      0.27       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "Amigosn['GSR'].fillna(0, inplace=True)  \n",
    "\n",
    "X = Amigosn[['GSR']]\n",
    "y = Amigosn['emotion_class']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create an MLP classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "\n",
    "# Create a stratified 10-fold cross-validator\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for i, (train_index, test_index) in enumerate(stratified_kfold.split(X_scaled, y)):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Fold {i+1} Classification Report:\")\n",
    "    fold_report = classification_report(y_test, mlp_classifier.predict(X_test), zero_division=1)\n",
    "    print(fold_report)\n",
    "\n",
    "average_report = classification_report(y, mlp_classifier.predict(X_scaled), zero_division=1)\n",
    "print(\"Average Classification Report:\")\n",
    "print(average_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f670cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming Amigosn is your DataFrame with 'GSR' as features and 'emotion_class' as the target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional but often recommended for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "946973dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 21ms/step - loss: 1.3926 - accuracy: 0.2766 - val_loss: 1.3862 - val_accuracy: 0.2625\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3835 - accuracy: 0.2922 - val_loss: 1.3843 - val_accuracy: 0.2750\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3787 - accuracy: 0.2812 - val_loss: 1.3841 - val_accuracy: 0.2625\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3774 - accuracy: 0.2828 - val_loss: 1.3848 - val_accuracy: 0.2625\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.3767 - accuracy: 0.2828 - val_loss: 1.3854 - val_accuracy: 0.2625\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 1.3760 - accuracy: 0.2844 - val_loss: 1.3853 - val_accuracy: 0.2562\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3756 - accuracy: 0.3000 - val_loss: 1.3852 - val_accuracy: 0.2562\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3757 - accuracy: 0.2969 - val_loss: 1.3853 - val_accuracy: 0.2562\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3756 - accuracy: 0.2953 - val_loss: 1.3851 - val_accuracy: 0.2562\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.3759 - accuracy: 0.2969 - val_loss: 1.3857 - val_accuracy: 0.2562\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.3857 - accuracy: 0.2562\n",
      "Accuracy: 25.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X = Amigosn[['GSR']]\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(Amigosn['emotion_class'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the RNN input shape\n",
    "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_reshaped, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test_one_hot))\n",
    "\n",
    "\n",
    "accuracy = model.evaluate(X_test_reshaped, y_test_one_hot)[1]\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31dcb1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 20ms/step - loss: 1.3889 - accuracy: 0.2208 - val_loss: 1.3820 - val_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3831 - accuracy: 0.2625 - val_loss: 1.3809 - val_accuracy: 0.2375\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3819 - accuracy: 0.2653 - val_loss: 1.3823 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3817 - accuracy: 0.2764 - val_loss: 1.3793 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3821 - accuracy: 0.2806 - val_loss: 1.3824 - val_accuracy: 0.2875\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3813 - accuracy: 0.2806 - val_loss: 1.3804 - val_accuracy: 0.2875\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3811 - accuracy: 0.2792 - val_loss: 1.3812 - val_accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3815 - accuracy: 0.2764 - val_loss: 1.3836 - val_accuracy: 0.2375\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3814 - accuracy: 0.2792 - val_loss: 1.3821 - val_accuracy: 0.2500\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3810 - accuracy: 0.2819 - val_loss: 1.3783 - val_accuracy: 0.2875\n",
      "Fold 1 Classification Report:\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00        20\n",
      "           1       0.29      1.00      0.45        23\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.29        80\n",
      "   macro avg       0.82      0.25      0.11        80\n",
      "weighted avg       0.80      0.29      0.13        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3818 - accuracy: 0.2694 - val_loss: 1.3865 - val_accuracy: 0.3375\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3805 - accuracy: 0.2708 - val_loss: 1.3913 - val_accuracy: 0.3000\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3800 - accuracy: 0.2736 - val_loss: 1.3930 - val_accuracy: 0.2875\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3800 - accuracy: 0.2625 - val_loss: 1.3922 - val_accuracy: 0.3375\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3798 - accuracy: 0.2625 - val_loss: 1.3953 - val_accuracy: 0.2875\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3800 - accuracy: 0.2625 - val_loss: 1.3909 - val_accuracy: 0.3250\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3797 - accuracy: 0.2750 - val_loss: 1.3920 - val_accuracy: 0.3000\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3789 - accuracy: 0.2736 - val_loss: 1.3928 - val_accuracy: 0.2875\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3815 - accuracy: 0.2681 - val_loss: 1.3933 - val_accuracy: 0.2875\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3801 - accuracy: 0.2569 - val_loss: 1.3915 - val_accuracy: 0.3375\n",
      "Fold 2 Classification Report:\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.20      0.27        20\n",
      "           1       0.33      1.00      0.49        23\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.34        80\n",
      "   macro avg       0.68      0.30      0.19        80\n",
      "weighted avg       0.66      0.34      0.21        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3820 - accuracy: 0.2847 - val_loss: 1.3754 - val_accuracy: 0.2875\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3811 - accuracy: 0.2764 - val_loss: 1.3781 - val_accuracy: 0.2750\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3809 - accuracy: 0.2750 - val_loss: 1.3760 - val_accuracy: 0.2750\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3814 - accuracy: 0.2750 - val_loss: 1.3774 - val_accuracy: 0.2750\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3809 - accuracy: 0.2819 - val_loss: 1.3765 - val_accuracy: 0.2750\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3812 - accuracy: 0.2764 - val_loss: 1.3770 - val_accuracy: 0.2750\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3816 - accuracy: 0.2778 - val_loss: 1.3756 - val_accuracy: 0.2750\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3818 - accuracy: 0.2722 - val_loss: 1.3763 - val_accuracy: 0.2750\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3821 - accuracy: 0.2694 - val_loss: 1.3763 - val_accuracy: 0.2750\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3812 - accuracy: 0.2792 - val_loss: 1.3788 - val_accuracy: 0.2750\n",
      "Fold 3 Classification Report:\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.16        21\n",
      "           1       0.26      0.91      0.41        22\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.28        80\n",
      "   macro avg       0.69      0.25      0.14        80\n",
      "weighted avg       0.67      0.28      0.15        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3794 - accuracy: 0.2708 - val_loss: 1.3943 - val_accuracy: 0.3250\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3790 - accuracy: 0.2722 - val_loss: 1.4075 - val_accuracy: 0.2875\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3780 - accuracy: 0.2750 - val_loss: 1.4036 - val_accuracy: 0.3000\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3778 - accuracy: 0.2639 - val_loss: 1.4092 - val_accuracy: 0.3125\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3779 - accuracy: 0.2722 - val_loss: 1.4080 - val_accuracy: 0.3000\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3778 - accuracy: 0.2736 - val_loss: 1.4077 - val_accuracy: 0.2750\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3778 - accuracy: 0.2792 - val_loss: 1.4089 - val_accuracy: 0.3000\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3782 - accuracy: 0.2681 - val_loss: 1.4083 - val_accuracy: 0.3125\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.2681 - val_loss: 1.4130 - val_accuracy: 0.2750\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3772 - accuracy: 0.2764 - val_loss: 1.4090 - val_accuracy: 0.2750\n",
      "Fold 4 Classification Report:\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00        21\n",
      "           1       0.28      1.00      0.43        22\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.28        80\n",
      "   macro avg       0.82      0.25      0.11        80\n",
      "weighted avg       0.80      0.28      0.12        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3815 - accuracy: 0.2750 - val_loss: 1.3789 - val_accuracy: 0.2875\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3801 - accuracy: 0.2750 - val_loss: 1.3792 - val_accuracy: 0.2875\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3798 - accuracy: 0.2806 - val_loss: 1.3788 - val_accuracy: 0.2875\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3796 - accuracy: 0.2778 - val_loss: 1.3789 - val_accuracy: 0.2875\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3795 - accuracy: 0.2792 - val_loss: 1.3792 - val_accuracy: 0.2875\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3795 - accuracy: 0.2792 - val_loss: 1.3805 - val_accuracy: 0.2750\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3797 - accuracy: 0.2847 - val_loss: 1.3787 - val_accuracy: 0.2875\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3801 - accuracy: 0.2764 - val_loss: 1.3785 - val_accuracy: 0.2875\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3797 - accuracy: 0.2806 - val_loss: 1.3807 - val_accuracy: 0.2750\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3811 - accuracy: 0.2819 - val_loss: 1.3800 - val_accuracy: 0.2750\n",
      "Fold 5 Classification Report:\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00        21\n",
      "           1       0.28      1.00      0.43        22\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.28        80\n",
      "   macro avg       0.82      0.25      0.11        80\n",
      "weighted avg       0.80      0.28      0.12        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3796 - accuracy: 0.2931 - val_loss: 1.3877 - val_accuracy: 0.2125\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3788 - accuracy: 0.2847 - val_loss: 1.3903 - val_accuracy: 0.2125\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3795 - accuracy: 0.2806 - val_loss: 1.3925 - val_accuracy: 0.2250\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3788 - accuracy: 0.2861 - val_loss: 1.3959 - val_accuracy: 0.2125\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3786 - accuracy: 0.2847 - val_loss: 1.3914 - val_accuracy: 0.2125\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3789 - accuracy: 0.2819 - val_loss: 1.3906 - val_accuracy: 0.2250\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3800 - accuracy: 0.2833 - val_loss: 1.3979 - val_accuracy: 0.2250\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3791 - accuracy: 0.2806 - val_loss: 1.3914 - val_accuracy: 0.2250\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3787 - accuracy: 0.2847 - val_loss: 1.3933 - val_accuracy: 0.2125\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.2833 - val_loss: 1.3962 - val_accuracy: 0.2125\n",
      "Fold 6 Classification Report:\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.10      0.12        21\n",
      "           1       0.22      0.68      0.34        22\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.21        80\n",
      "   macro avg       0.59      0.19      0.11        80\n",
      "weighted avg       0.56      0.21      0.12        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3801 - accuracy: 0.2833 - val_loss: 1.3810 - val_accuracy: 0.2250\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3789 - accuracy: 0.2861 - val_loss: 1.3857 - val_accuracy: 0.2250\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.2833 - val_loss: 1.3843 - val_accuracy: 0.2250\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3792 - accuracy: 0.2819 - val_loss: 1.3852 - val_accuracy: 0.2250\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.2778 - val_loss: 1.3865 - val_accuracy: 0.2250\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3788 - accuracy: 0.2819 - val_loss: 1.3840 - val_accuracy: 0.2250\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3790 - accuracy: 0.2819 - val_loss: 1.3877 - val_accuracy: 0.2250\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3794 - accuracy: 0.2819 - val_loss: 1.3845 - val_accuracy: 0.2250\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3789 - accuracy: 0.2806 - val_loss: 1.3855 - val_accuracy: 0.2125\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3792 - accuracy: 0.2806 - val_loss: 1.3847 - val_accuracy: 0.2250\n",
      "Fold 7 Classification Report:\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.10      0.11        21\n",
      "           1       0.25      0.73      0.37        22\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.23        80\n",
      "   macro avg       0.59      0.21      0.12        80\n",
      "weighted avg       0.56      0.23      0.13        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3790 - accuracy: 0.2806 - val_loss: 1.3811 - val_accuracy: 0.2375\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3791 - accuracy: 0.2833 - val_loss: 1.3793 - val_accuracy: 0.2500\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3793 - accuracy: 0.2806 - val_loss: 1.3792 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3799 - accuracy: 0.2806 - val_loss: 1.3787 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3790 - accuracy: 0.2806 - val_loss: 1.3792 - val_accuracy: 0.2500\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3798 - accuracy: 0.2778 - val_loss: 1.3787 - val_accuracy: 0.2500\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3794 - accuracy: 0.2819 - val_loss: 1.3802 - val_accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3797 - accuracy: 0.2778 - val_loss: 1.3780 - val_accuracy: 0.2375\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3794 - accuracy: 0.2806 - val_loss: 1.3791 - val_accuracy: 0.2500\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3806 - accuracy: 0.2847 - val_loss: 1.3797 - val_accuracy: 0.2375\n",
      "Fold 8 Classification Report:\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.14      0.16        21\n",
      "           1       0.25      0.70      0.37        23\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.24        80\n",
      "   macro avg       0.61      0.21      0.13        80\n",
      "weighted avg       0.57      0.24      0.15        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.3802 - accuracy: 0.2833 - val_loss: 1.3682 - val_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3803 - accuracy: 0.2819 - val_loss: 1.3718 - val_accuracy: 0.2625\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3795 - accuracy: 0.2764 - val_loss: 1.3739 - val_accuracy: 0.2625\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3803 - accuracy: 0.2819 - val_loss: 1.3753 - val_accuracy: 0.2625\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3798 - accuracy: 0.2778 - val_loss: 1.3819 - val_accuracy: 0.2750\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3803 - accuracy: 0.2708 - val_loss: 1.3743 - val_accuracy: 0.2500\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3794 - accuracy: 0.2792 - val_loss: 1.3752 - val_accuracy: 0.2625\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3807 - accuracy: 0.2792 - val_loss: 1.3819 - val_accuracy: 0.2750\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3814 - accuracy: 0.2778 - val_loss: 1.3726 - val_accuracy: 0.2750\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3804 - accuracy: 0.2806 - val_loss: 1.3746 - val_accuracy: 0.2625\n",
      "Fold 9 Classification Report:\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.14      0.21        21\n",
      "           1       0.25      0.78      0.38        23\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.26        80\n",
      "   macro avg       0.66      0.23      0.15        80\n",
      "weighted avg       0.62      0.26      0.16        80\n",
      "\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3792 - accuracy: 0.2764 - val_loss: 1.3764 - val_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.3792 - accuracy: 0.2792 - val_loss: 1.3768 - val_accuracy: 0.2750\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3790 - accuracy: 0.2778 - val_loss: 1.3762 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3791 - accuracy: 0.2778 - val_loss: 1.3761 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3793 - accuracy: 0.2833 - val_loss: 1.3751 - val_accuracy: 0.2625\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3792 - accuracy: 0.2750 - val_loss: 1.3767 - val_accuracy: 0.2875\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3787 - accuracy: 0.2778 - val_loss: 1.3760 - val_accuracy: 0.2750\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3793 - accuracy: 0.2764 - val_loss: 1.3759 - val_accuracy: 0.2750\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.2819 - val_loss: 1.3757 - val_accuracy: 0.2625\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3786 - accuracy: 0.2819 - val_loss: 1.3758 - val_accuracy: 0.2750\n",
      "Fold 10 Classification Report:\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.14      0.19        21\n",
      "           1       0.28      0.83      0.41        23\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.28        80\n",
      "   macro avg       0.64      0.24      0.15        80\n",
      "weighted avg       0.60      0.28      0.17        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "Amigosn['GSR'].fillna(0, inplace=True)  \n",
    "\n",
    "X = Amigosn[['GSR']]\n",
    "y = Amigosn['emotion_class']\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# Create an RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, input_shape=(X_scaled.shape[1], 1)))  \n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a stratified 10-fold cross-validator\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(stratified_kfold.split(X_scaled, y_encoded)):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y_one_hot[train_index], y_one_hot[test_index]\n",
    "\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    print(f\"Fold {i+1} Classification Report:\")\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_test_argmax = np.argmax(y_test, axis=1)\n",
    "    fold_report = classification_report(y_test_argmax, y_pred, zero_division=1)\n",
    "    print(fold_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89948a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7908f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a861d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00beba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d0c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd052f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4afe5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Amigos0.fillna(0, inplace=True)\n",
    "\n",
    "shape_of_gsr_column = Amigos0['GSR'].shape\n",
    "\n",
    "# If you want to print the shape\n",
    "print(shape_of_gsr_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91c05af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the split DataFrame: (20, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window_1</th>\n",
       "      <th>Window_2</th>\n",
       "      <th>Window_3</th>\n",
       "      <th>Window_4</th>\n",
       "      <th>Window_5</th>\n",
       "      <th>Window_6</th>\n",
       "      <th>Window_7</th>\n",
       "      <th>Window_8</th>\n",
       "      <th>Window_9</th>\n",
       "      <th>Window_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Window_31</th>\n",
       "      <th>Window_32</th>\n",
       "      <th>Window_33</th>\n",
       "      <th>Window_34</th>\n",
       "      <th>Window_35</th>\n",
       "      <th>Window_36</th>\n",
       "      <th>Window_37</th>\n",
       "      <th>Window_38</th>\n",
       "      <th>Window_39</th>\n",
       "      <th>Window_40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.371476</td>\n",
       "      <td>0.159238</td>\n",
       "      <td>0.798359</td>\n",
       "      <td>0.778601</td>\n",
       "      <td>0.199079</td>\n",
       "      <td>0.994547</td>\n",
       "      <td>0.122545</td>\n",
       "      <td>0.060598</td>\n",
       "      <td>0.805706</td>\n",
       "      <td>0.321114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298077</td>\n",
       "      <td>0.714319</td>\n",
       "      <td>0.174198</td>\n",
       "      <td>0.235794</td>\n",
       "      <td>0.699669</td>\n",
       "      <td>0.932851</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.500446</td>\n",
       "      <td>0.794683</td>\n",
       "      <td>0.373729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555142</td>\n",
       "      <td>0.055773</td>\n",
       "      <td>0.651752</td>\n",
       "      <td>0.874307</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.411816</td>\n",
       "      <td>0.136722</td>\n",
       "      <td>0.055812</td>\n",
       "      <td>0.247154</td>\n",
       "      <td>0.999557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787061</td>\n",
       "      <td>0.218482</td>\n",
       "      <td>0.460721</td>\n",
       "      <td>0.397555</td>\n",
       "      <td>0.727663</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.050598</td>\n",
       "      <td>0.704773</td>\n",
       "      <td>0.406761</td>\n",
       "      <td>0.255216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.369729</td>\n",
       "      <td>0.546151</td>\n",
       "      <td>0.111888</td>\n",
       "      <td>0.759658</td>\n",
       "      <td>0.583902</td>\n",
       "      <td>0.239263</td>\n",
       "      <td>0.228846</td>\n",
       "      <td>0.165441</td>\n",
       "      <td>0.974631</td>\n",
       "      <td>0.477552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149978</td>\n",
       "      <td>0.402357</td>\n",
       "      <td>0.490052</td>\n",
       "      <td>0.345743</td>\n",
       "      <td>0.894788</td>\n",
       "      <td>0.998431</td>\n",
       "      <td>0.636524</td>\n",
       "      <td>0.284724</td>\n",
       "      <td>0.168229</td>\n",
       "      <td>0.568192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.903738</td>\n",
       "      <td>0.797701</td>\n",
       "      <td>0.685858</td>\n",
       "      <td>0.519009</td>\n",
       "      <td>0.612558</td>\n",
       "      <td>0.335679</td>\n",
       "      <td>0.310472</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.691764</td>\n",
       "      <td>0.199874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043797</td>\n",
       "      <td>0.825708</td>\n",
       "      <td>0.277817</td>\n",
       "      <td>0.934376</td>\n",
       "      <td>0.221224</td>\n",
       "      <td>0.131729</td>\n",
       "      <td>0.514049</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.507433</td>\n",
       "      <td>0.396183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.525882</td>\n",
       "      <td>0.938147</td>\n",
       "      <td>0.503066</td>\n",
       "      <td>0.379195</td>\n",
       "      <td>0.260118</td>\n",
       "      <td>0.879924</td>\n",
       "      <td>0.381199</td>\n",
       "      <td>0.765898</td>\n",
       "      <td>0.821121</td>\n",
       "      <td>0.982754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179364</td>\n",
       "      <td>0.267653</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.365919</td>\n",
       "      <td>0.298319</td>\n",
       "      <td>0.904722</td>\n",
       "      <td>0.974399</td>\n",
       "      <td>0.443342</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.274488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Window_1  Window_2  Window_3  Window_4  Window_5  Window_6  Window_7  \\\n",
       "0  0.371476  0.159238  0.798359  0.778601  0.199079  0.994547  0.122545   \n",
       "1  0.555142  0.055773  0.651752  0.874307  0.012255  0.411816  0.136722   \n",
       "2  0.369729  0.546151  0.111888  0.759658  0.583902  0.239263  0.228846   \n",
       "3  0.903738  0.797701  0.685858  0.519009  0.612558  0.335679  0.310472   \n",
       "4  0.525882  0.938147  0.503066  0.379195  0.260118  0.879924  0.381199   \n",
       "\n",
       "   Window_8  Window_9  Window_10  ...  Window_31  Window_32  Window_33  \\\n",
       "0  0.060598  0.805706   0.321114  ...   0.298077   0.714319   0.174198   \n",
       "1  0.055812  0.247154   0.999557  ...   0.787061   0.218482   0.460721   \n",
       "2  0.165441  0.974631   0.477552  ...   0.149978   0.402357   0.490052   \n",
       "3  0.541300  0.691764   0.199874  ...   0.043797   0.825708   0.277817   \n",
       "4  0.765898  0.821121   0.982754  ...   0.179364   0.267653   0.024085   \n",
       "\n",
       "   Window_34  Window_35  Window_36  Window_37  Window_38  Window_39  Window_40  \n",
       "0   0.235794   0.699669   0.932851   0.301961   0.500446   0.794683   0.373729  \n",
       "1   0.397555   0.727663   0.014532   0.050598   0.704773   0.406761   0.255216  \n",
       "2   0.345743   0.894788   0.998431   0.636524   0.284724   0.168229   0.568192  \n",
       "3   0.934376   0.221224   0.131729   0.514049   0.607323   0.507433   0.396183  \n",
       "4   0.365919   0.298319   0.904722   0.974399   0.443342   0.479700   0.274488  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Amigos0 = pd.DataFrame({'GSR': np.random.rand(800)})\n",
    "\n",
    "original_data = Amigos0['GSR'].values\n",
    "\n",
    "\n",
    "num_columns = 40\n",
    "column_size = original_data.shape[0] // num_columns\n",
    "\n",
    "\n",
    "total_elements = num_columns * column_size\n",
    "\n",
    "\n",
    "split_data = original_data[:total_elements].reshape((column_size, num_columns), order='F')\n",
    "\n",
    "split_dataframe = pd.DataFrame(split_data, columns=[f'Window_{i+1}' for i in range(num_columns)])\n",
    "\n",
    "print(\"Shape of the split DataFrame:\", split_dataframe.shape)\n",
    "\n",
    "split_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46792a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the split DataFrame: (40, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window_1</th>\n",
       "      <th>Window_2</th>\n",
       "      <th>Window_3</th>\n",
       "      <th>Window_4</th>\n",
       "      <th>Window_5</th>\n",
       "      <th>Window_6</th>\n",
       "      <th>Window_7</th>\n",
       "      <th>Window_8</th>\n",
       "      <th>Window_9</th>\n",
       "      <th>Window_10</th>\n",
       "      <th>Window_11</th>\n",
       "      <th>Window_12</th>\n",
       "      <th>Window_13</th>\n",
       "      <th>Window_14</th>\n",
       "      <th>Window_15</th>\n",
       "      <th>Window_16</th>\n",
       "      <th>Window_17</th>\n",
       "      <th>Window_18</th>\n",
       "      <th>Window_19</th>\n",
       "      <th>Window_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238895</td>\n",
       "      <td>0.246630</td>\n",
       "      <td>0.213976</td>\n",
       "      <td>0.533891</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>0.284714</td>\n",
       "      <td>0.268959</td>\n",
       "      <td>0.365694</td>\n",
       "      <td>0.900297</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.925739</td>\n",
       "      <td>0.571963</td>\n",
       "      <td>0.126457</td>\n",
       "      <td>0.408436</td>\n",
       "      <td>0.157572</td>\n",
       "      <td>0.454229</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>0.500687</td>\n",
       "      <td>0.635843</td>\n",
       "      <td>0.719064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.789488</td>\n",
       "      <td>0.781841</td>\n",
       "      <td>0.996930</td>\n",
       "      <td>0.115644</td>\n",
       "      <td>0.138754</td>\n",
       "      <td>0.542485</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.443173</td>\n",
       "      <td>0.555457</td>\n",
       "      <td>0.057279</td>\n",
       "      <td>0.120476</td>\n",
       "      <td>0.602893</td>\n",
       "      <td>0.653911</td>\n",
       "      <td>0.858555</td>\n",
       "      <td>0.821035</td>\n",
       "      <td>0.246408</td>\n",
       "      <td>0.321980</td>\n",
       "      <td>0.252170</td>\n",
       "      <td>0.572884</td>\n",
       "      <td>0.065426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.635736</td>\n",
       "      <td>0.882934</td>\n",
       "      <td>0.095007</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.507468</td>\n",
       "      <td>0.358724</td>\n",
       "      <td>0.128669</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.331043</td>\n",
       "      <td>0.515094</td>\n",
       "      <td>0.484610</td>\n",
       "      <td>0.081092</td>\n",
       "      <td>0.163506</td>\n",
       "      <td>0.371582</td>\n",
       "      <td>0.345651</td>\n",
       "      <td>0.294347</td>\n",
       "      <td>0.972863</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.911957</td>\n",
       "      <td>0.751240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.498685</td>\n",
       "      <td>0.628145</td>\n",
       "      <td>0.399772</td>\n",
       "      <td>0.803161</td>\n",
       "      <td>0.079196</td>\n",
       "      <td>0.469381</td>\n",
       "      <td>0.887119</td>\n",
       "      <td>0.998961</td>\n",
       "      <td>0.992282</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.403081</td>\n",
       "      <td>0.511179</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.045673</td>\n",
       "      <td>0.985205</td>\n",
       "      <td>0.885580</td>\n",
       "      <td>0.865369</td>\n",
       "      <td>0.894569</td>\n",
       "      <td>0.542654</td>\n",
       "      <td>0.580197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.772553</td>\n",
       "      <td>0.506361</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.289987</td>\n",
       "      <td>0.625864</td>\n",
       "      <td>0.246779</td>\n",
       "      <td>0.849890</td>\n",
       "      <td>0.200859</td>\n",
       "      <td>0.952299</td>\n",
       "      <td>0.692833</td>\n",
       "      <td>0.485759</td>\n",
       "      <td>0.854709</td>\n",
       "      <td>0.049992</td>\n",
       "      <td>0.104276</td>\n",
       "      <td>0.234678</td>\n",
       "      <td>0.290986</td>\n",
       "      <td>0.335773</td>\n",
       "      <td>0.337252</td>\n",
       "      <td>0.421959</td>\n",
       "      <td>0.600334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Window_1  Window_2  Window_3  Window_4  Window_5  Window_6  Window_7  \\\n",
       "0  0.238895  0.246630  0.213976  0.533891  0.012933  0.284714  0.268959   \n",
       "1  0.789488  0.781841  0.996930  0.115644  0.138754  0.542485  0.851237   \n",
       "2  0.635736  0.882934  0.095007  0.072721  0.507468  0.358724  0.128669   \n",
       "3  0.498685  0.628145  0.399772  0.803161  0.079196  0.469381  0.887119   \n",
       "4  0.772553  0.506361  0.001959  0.289987  0.625864  0.246779  0.849890   \n",
       "\n",
       "   Window_8  Window_9  Window_10  Window_11  Window_12  Window_13  Window_14  \\\n",
       "0  0.365694  0.900297   0.046200   0.925739   0.571963   0.126457   0.408436   \n",
       "1  0.443173  0.555457   0.057279   0.120476   0.602893   0.653911   0.858555   \n",
       "2  0.157001  0.331043   0.515094   0.484610   0.081092   0.163506   0.371582   \n",
       "3  0.998961  0.992282   0.006310   0.403081   0.511179   0.929762   0.045673   \n",
       "4  0.200859  0.952299   0.692833   0.485759   0.854709   0.049992   0.104276   \n",
       "\n",
       "   Window_15  Window_16  Window_17  Window_18  Window_19  Window_20  \n",
       "0   0.157572   0.454229   0.008612   0.500687   0.635843   0.719064  \n",
       "1   0.821035   0.246408   0.321980   0.252170   0.572884   0.065426  \n",
       "2   0.345651   0.294347   0.972863   0.017136   0.911957   0.751240  \n",
       "3   0.985205   0.885580   0.865369   0.894569   0.542654   0.580197  \n",
       "4   0.234678   0.290986   0.335773   0.337252   0.421959   0.600334  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Amigos0 = pd.DataFrame({'GSR': np.random.rand(800)})\n",
    "\n",
    "# Extract the 'GSR' column\n",
    "original_data = Amigos0['GSR'].values\n",
    "\n",
    "# Define the number of columns and the size of each column\n",
    "num_columns = 40\n",
    "column_size = original_data.shape[0] // num_columns\n",
    "\n",
    "# Calculate the number of elements to keep\n",
    "total_elements = num_columns * column_size\n",
    "\n",
    "# Reshape the 'GSR' column into a 2D array with 40 columns and 800 rows\n",
    "split_data = original_data[:total_elements].reshape((num_columns, column_size), order='F')\n",
    "\n",
    "# Convert the result back to a DataFrame if needed\n",
    "split_dataframe = pd.DataFrame(split_data, columns=[f'Window_{i+1}' for i in range(column_size)])\n",
    "\n",
    "print(\"Shape of the split DataFrame:\", split_dataframe.shape)\n",
    "\n",
    "split_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d5527c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about the DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 800 entries, 0 to 799\n",
      "Data columns (total 40 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Window_1   800 non-null    float64\n",
      " 1   Window_2   800 non-null    float64\n",
      " 2   Window_3   800 non-null    float64\n",
      " 3   Window_4   800 non-null    float64\n",
      " 4   Window_5   800 non-null    float64\n",
      " 5   Window_6   800 non-null    float64\n",
      " 6   Window_7   800 non-null    float64\n",
      " 7   Window_8   800 non-null    float64\n",
      " 8   Window_9   800 non-null    float64\n",
      " 9   Window_10  800 non-null    float64\n",
      " 10  Window_11  800 non-null    float64\n",
      " 11  Window_12  800 non-null    float64\n",
      " 12  Window_13  800 non-null    float64\n",
      " 13  Window_14  800 non-null    float64\n",
      " 14  Window_15  800 non-null    float64\n",
      " 15  Window_16  800 non-null    float64\n",
      " 16  Window_17  800 non-null    float64\n",
      " 17  Window_18  800 non-null    float64\n",
      " 18  Window_19  800 non-null    float64\n",
      " 19  Window_20  800 non-null    float64\n",
      " 20  Window_21  800 non-null    float64\n",
      " 21  Window_22  800 non-null    float64\n",
      " 22  Window_23  800 non-null    float64\n",
      " 23  Window_24  800 non-null    float64\n",
      " 24  Window_25  800 non-null    float64\n",
      " 25  Window_26  800 non-null    float64\n",
      " 26  Window_27  800 non-null    float64\n",
      " 27  Window_28  800 non-null    float64\n",
      " 28  Window_29  800 non-null    float64\n",
      " 29  Window_30  800 non-null    float64\n",
      " 30  Window_31  800 non-null    float64\n",
      " 31  Window_32  800 non-null    float64\n",
      " 32  Window_33  800 non-null    float64\n",
      " 33  Window_34  800 non-null    float64\n",
      " 34  Window_35  800 non-null    float64\n",
      " 35  Window_36  800 non-null    float64\n",
      " 36  Window_37  800 non-null    float64\n",
      " 37  Window_38  800 non-null    float64\n",
      " 38  Window_39  800 non-null    float64\n",
      " 39  Window_40  800 non-null    float64\n",
      "dtypes: float64(40)\n",
      "memory usage: 250.1 KB\n",
      "\n",
      "Basic statistics for each column:\n",
      "         Window_1    Window_2    Window_3    Window_4    Window_5    Window_6  \\\n",
      "count  800.000000  800.000000  800.000000  800.000000  800.000000  800.000000   \n",
      "mean     0.500231    0.497287    0.507207    0.490195    0.502639    0.491941   \n",
      "std      0.285619    0.288127    0.287656    0.285060    0.292347    0.281063   \n",
      "min      0.000083    0.002749    0.000175    0.000454    0.000710    0.000077   \n",
      "25%      0.246875    0.248307    0.279468    0.231846    0.256790    0.253931   \n",
      "50%      0.509645    0.497520    0.494121    0.508802    0.494848    0.500628   \n",
      "75%      0.746802    0.744379    0.758585    0.722063    0.759997    0.726551   \n",
      "max      0.995659    0.999653    0.997674    0.998788    0.996829    0.997794   \n",
      "\n",
      "         Window_7    Window_8    Window_9   Window_10  ...   Window_31  \\\n",
      "count  800.000000  800.000000  800.000000  800.000000  ...  800.000000   \n",
      "mean     0.492884    0.480963    0.488215    0.507416  ...    0.486820   \n",
      "std      0.289042    0.292507    0.292705    0.281590  ...    0.289138   \n",
      "min      0.003320    0.000123    0.000796    0.003155  ...    0.000429   \n",
      "25%      0.243553    0.221978    0.230141    0.265822  ...    0.230055   \n",
      "50%      0.487005    0.461641    0.475878    0.497722  ...    0.476450   \n",
      "75%      0.737801    0.739028    0.744035    0.760904  ...    0.738719   \n",
      "max      0.999941    0.999292    0.999885    0.999461  ...    0.998715   \n",
      "\n",
      "        Window_32   Window_33   Window_34   Window_35   Window_36   Window_37  \\\n",
      "count  800.000000  800.000000  800.000000  800.000000  800.000000  800.000000   \n",
      "mean     0.525498    0.495011    0.502601    0.510563    0.507860    0.495302   \n",
      "std      0.289478    0.286945    0.285516    0.283800    0.287620    0.289604   \n",
      "min      0.001166    0.000527    0.002039    0.001213    0.001060    0.001294   \n",
      "25%      0.286493    0.238952    0.265623    0.272997    0.264272    0.235553   \n",
      "50%      0.549048    0.497623    0.520770    0.514683    0.510915    0.490315   \n",
      "75%      0.781502    0.739831    0.738724    0.758177    0.760106    0.743011   \n",
      "max      0.999078    0.997951    0.999315    0.995587    0.995175    0.999137   \n",
      "\n",
      "        Window_38   Window_39   Window_40  \n",
      "count  800.000000  800.000000  800.000000  \n",
      "mean     0.491951    0.499038    0.507092  \n",
      "std      0.287424    0.294504    0.284248  \n",
      "min      0.001115    0.000013    0.004601  \n",
      "25%      0.260506    0.246131    0.265966  \n",
      "50%      0.486618    0.484224    0.514827  \n",
      "75%      0.734485    0.742174    0.752912  \n",
      "max      0.999855    0.999446    0.999369  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_dataframe = pd.DataFrame(np.random.rand(800, 40), columns=[f'Window_{i+1}' for i in range(40)])\n",
    "\n",
    "print(\"Information about the DataFrame:\")\n",
    "split_dataframe.info()\n",
    "\n",
    "# Print basic statistics for each column\n",
    "print(\"\\nBasic statistics for each column:\")\n",
    "print(split_dataframe.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8d9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bad5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window_1</th>\n",
       "      <th>Window_2</th>\n",
       "      <th>Window_3</th>\n",
       "      <th>Window_4</th>\n",
       "      <th>Window_5</th>\n",
       "      <th>Window_6</th>\n",
       "      <th>Window_7</th>\n",
       "      <th>Window_8</th>\n",
       "      <th>Window_9</th>\n",
       "      <th>Window_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Window_32</th>\n",
       "      <th>Window_33</th>\n",
       "      <th>Window_34</th>\n",
       "      <th>Window_35</th>\n",
       "      <th>Window_36</th>\n",
       "      <th>Window_37</th>\n",
       "      <th>Window_38</th>\n",
       "      <th>Window_39</th>\n",
       "      <th>Window_40</th>\n",
       "      <th>emotion_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244164</td>\n",
       "      <td>0.367428</td>\n",
       "      <td>0.873876</td>\n",
       "      <td>0.976729</td>\n",
       "      <td>0.153250</td>\n",
       "      <td>0.605209</td>\n",
       "      <td>0.108686</td>\n",
       "      <td>0.801651</td>\n",
       "      <td>0.335198</td>\n",
       "      <td>0.849942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665578</td>\n",
       "      <td>0.088934</td>\n",
       "      <td>0.910870</td>\n",
       "      <td>0.502104</td>\n",
       "      <td>0.182869</td>\n",
       "      <td>0.737821</td>\n",
       "      <td>0.315430</td>\n",
       "      <td>0.923120</td>\n",
       "      <td>0.584073</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.961587</td>\n",
       "      <td>0.254154</td>\n",
       "      <td>0.218610</td>\n",
       "      <td>0.788110</td>\n",
       "      <td>0.101019</td>\n",
       "      <td>0.712271</td>\n",
       "      <td>0.563756</td>\n",
       "      <td>0.074190</td>\n",
       "      <td>0.381924</td>\n",
       "      <td>0.266135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251436</td>\n",
       "      <td>0.501830</td>\n",
       "      <td>0.603531</td>\n",
       "      <td>0.417416</td>\n",
       "      <td>0.065965</td>\n",
       "      <td>0.336120</td>\n",
       "      <td>0.256331</td>\n",
       "      <td>0.327661</td>\n",
       "      <td>0.745316</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.825172</td>\n",
       "      <td>0.242269</td>\n",
       "      <td>0.742615</td>\n",
       "      <td>0.343265</td>\n",
       "      <td>0.506608</td>\n",
       "      <td>0.854613</td>\n",
       "      <td>0.331527</td>\n",
       "      <td>0.089749</td>\n",
       "      <td>0.931990</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358902</td>\n",
       "      <td>0.644692</td>\n",
       "      <td>0.399309</td>\n",
       "      <td>0.687301</td>\n",
       "      <td>0.677163</td>\n",
       "      <td>0.775469</td>\n",
       "      <td>0.258628</td>\n",
       "      <td>0.027031</td>\n",
       "      <td>0.393989</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.276954</td>\n",
       "      <td>0.185419</td>\n",
       "      <td>0.903338</td>\n",
       "      <td>0.157714</td>\n",
       "      <td>0.607269</td>\n",
       "      <td>0.082326</td>\n",
       "      <td>0.126768</td>\n",
       "      <td>0.164352</td>\n",
       "      <td>0.425186</td>\n",
       "      <td>0.310442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666547</td>\n",
       "      <td>0.884870</td>\n",
       "      <td>0.841180</td>\n",
       "      <td>0.620925</td>\n",
       "      <td>0.598034</td>\n",
       "      <td>0.496172</td>\n",
       "      <td>0.960074</td>\n",
       "      <td>0.568640</td>\n",
       "      <td>0.281606</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154846</td>\n",
       "      <td>0.860416</td>\n",
       "      <td>0.480649</td>\n",
       "      <td>0.709538</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.180486</td>\n",
       "      <td>0.070245</td>\n",
       "      <td>0.073818</td>\n",
       "      <td>0.341293</td>\n",
       "      <td>0.569903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067736</td>\n",
       "      <td>0.199375</td>\n",
       "      <td>0.889369</td>\n",
       "      <td>0.132336</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.342492</td>\n",
       "      <td>0.027386</td>\n",
       "      <td>0.536744</td>\n",
       "      <td>0.453046</td>\n",
       "      <td>HALV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Window_1  Window_2  Window_3  Window_4  Window_5  Window_6  Window_7  \\\n",
       "0  0.244164  0.367428  0.873876  0.976729  0.153250  0.605209  0.108686   \n",
       "1  0.961587  0.254154  0.218610  0.788110  0.101019  0.712271  0.563756   \n",
       "2  0.825172  0.242269  0.742615  0.343265  0.506608  0.854613  0.331527   \n",
       "3  0.276954  0.185419  0.903338  0.157714  0.607269  0.082326  0.126768   \n",
       "4  0.154846  0.860416  0.480649  0.709538  0.940594  0.180486  0.070245   \n",
       "\n",
       "   Window_8  Window_9  Window_10  ...  Window_32  Window_33  Window_34  \\\n",
       "0  0.801651  0.335198   0.849942  ...   0.665578   0.088934   0.910870   \n",
       "1  0.074190  0.381924   0.266135  ...   0.251436   0.501830   0.603531   \n",
       "2  0.089749  0.931990   0.065322  ...   0.358902   0.644692   0.399309   \n",
       "3  0.164352  0.425186   0.310442  ...   0.666547   0.884870   0.841180   \n",
       "4  0.073818  0.341293   0.569903  ...   0.067736   0.199375   0.889369   \n",
       "\n",
       "   Window_35  Window_36  Window_37  Window_38  Window_39  Window_40  \\\n",
       "0   0.502104   0.182869   0.737821   0.315430   0.923120   0.584073   \n",
       "1   0.417416   0.065965   0.336120   0.256331   0.327661   0.745316   \n",
       "2   0.687301   0.677163   0.775469   0.258628   0.027031   0.393989   \n",
       "3   0.620925   0.598034   0.496172   0.960074   0.568640   0.281606   \n",
       "4   0.132336   0.203200   0.342492   0.027386   0.536744   0.453046   \n",
       "\n",
       "   emotion_class  \n",
       "0           LALV  \n",
       "1           LAHV  \n",
       "2           LALV  \n",
       "3           LAHV  \n",
       "4           HALV  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df = pd.concat([split_dataframe, Amigosn['emotion_class']], axis=1)\n",
    "\n",
    "concatenated_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e828974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da1e6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "20/20 [==============================] - 1s 14ms/step - loss: -2.9870 - accuracy: 0.2844 - val_loss: -8.4822 - val_accuracy: 0.2750\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: -13.0762 - accuracy: 0.2828 - val_loss: -25.6336 - val_accuracy: 0.2750\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 5ms/step - loss: -32.3902 - accuracy: 0.2828 - val_loss: -57.5982 - val_accuracy: 0.2750\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 5ms/step - loss: -68.3320 - accuracy: 0.2828 - val_loss: -113.3859 - val_accuracy: 0.2750\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: -129.2636 - accuracy: 0.2828 - val_loss: -202.0561 - val_accuracy: 0.2750\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: -223.8547 - accuracy: 0.2828 - val_loss: -339.2166 - val_accuracy: 0.2750\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 5ms/step - loss: -365.7975 - accuracy: 0.2828 - val_loss: -539.3327 - val_accuracy: 0.2750\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: -559.8423 - accuracy: 0.2828 - val_loss: -829.3445 - val_accuracy: 0.2750\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 5ms/step - loss: -844.0378 - accuracy: 0.2828 - val_loss: -1190.1565 - val_accuracy: 0.2750\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: -1192.4846 - accuracy: 0.2828 - val_loss: -1675.1338 - val_accuracy: 0.2750\n",
      "5/5 [==============================] - 0s 4ms/step - loss: -1675.1338 - accuracy: 0.2750\n",
      "Test Loss: -1675.1337890625, Test Accuracy: 0.2750000059604645\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y = concatenated_df['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f5553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75192220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 1s 15ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "5/5 [==============================] - 0s 0s/step - loss: nan - accuracy: 0.2313\n",
      "Test Loss: nan, Test Accuracy: 0.23125000298023224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y = concatenated_df['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Data Augmentation for 1D data\n",
    "datagen = Sequential([\n",
    "    Dropout(0.1), \n",
    "])\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "custom_optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Class Weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model with data augmentation and class weights\n",
    "model.fit(datagen(X_train), y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weight_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e14ee64d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 1s 15ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 9.0484e-05\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 8.1873e-05\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 7.4082e-05\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.7032e-05\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.0653e-05\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 5.4881e-05\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 4.9659e-05\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 4.4933e-05\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 4.0657e-05\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 3.6788e-05\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 3.3287e-05\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 3.0119e-05\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.7253e-05\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.4660e-05\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.2313e-05\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.0190e-05\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.8268e-05\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.6530e-05\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.4957e-05\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.3534e-05\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.2246e-05\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.1080e-05\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.0026e-05\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 9.0718e-06\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 8.2085e-06\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 7.4274e-06\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.7206e-06\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.0810e-06\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 5.5023e-06\n",
      "5/5 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.2313\n",
      "Test Loss: nan, Test Accuracy: 0.23125000298023224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Data Preprocessing\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y = concatenated_df['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Class Weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Data Augmentation for 1D data\n",
    "datagen = Sequential([\n",
    "    Dropout(0.1), \n",
    "])\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "custom_optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.0001 * np.exp(-0.1 * epoch)\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with data augmentation and class weights\n",
    "model.fit(datagen(X_train), y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weight_dict, callbacks=[lr_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed726897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 2s 27ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 9.0484e-05\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 8.1873e-05\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 7.4082e-05\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.7032e-05\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.0653e-05\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 5.4881e-05\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 4.9659e-05\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 4.4933e-05\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 4.0657e-05\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 3.6788e-05\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 3.3287e-05\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 3.0119e-05\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.7253e-05\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.4660e-05\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.2313e-05\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 2.0190e-05\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.8268e-05\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.6530e-05\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.4957e-05\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.3534e-05\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.2246e-05\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.1080e-05\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 1.0026e-05\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 9.0718e-06\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 8.2085e-06\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 7.4274e-06\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.7206e-06\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 6.0810e-06\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313 - lr: 5.5023e-06\n",
      "5/5 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.2313\n",
      "Test Loss: nan, Test Accuracy: 0.23125000298023224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y = concatenated_df['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Class Weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Data Augmentation for 1D data\n",
    "datagen = Sequential([\n",
    "    Dropout(0.1),  \n",
    "])\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.0001 * np.exp(-0.1 * epoch)\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model with data augmentation and class weights\n",
    "model.fit(datagen(X_train), y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weight_dict, callbacks=[lr_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0af9ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_26816\\508485487.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not all(X.applymap(lambda x: isinstance(x, (int, float))).all()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 8ms/step - loss: 1.3943 - accuracy: 0.2984 - val_loss: 1.3982 - val_accuracy: 0.2625\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.3646 - accuracy: 0.3266 - val_loss: 1.4024 - val_accuracy: 0.2375\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.3469 - accuracy: 0.3375 - val_loss: 1.4012 - val_accuracy: 0.2688\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.3337 - accuracy: 0.3828 - val_loss: 1.4028 - val_accuracy: 0.2812\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3138 - accuracy: 0.4031 - val_loss: 1.4152 - val_accuracy: 0.2438\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2978 - accuracy: 0.4141 - val_loss: 1.4158 - val_accuracy: 0.2562\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2812 - accuracy: 0.4156 - val_loss: 1.4363 - val_accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2646 - accuracy: 0.4109 - val_loss: 1.4303 - val_accuracy: 0.2500\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2532 - accuracy: 0.4328 - val_loss: 1.4911 - val_accuracy: 0.2500\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.2401 - accuracy: 0.4484 - val_loss: 1.4507 - val_accuracy: 0.2688\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.26875\n",
      "Confusion Matrix:\n",
      "[[10 14  6  7]\n",
      " [11 20 12  1]\n",
      " [13 20  7  5]\n",
      " [ 7 18  3  6]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Ensure all data types are numeric\n",
    "if not all(X.applymap(lambda x: isinstance(x, (int, float))).all()):\n",
    "    raise ValueError(\"Not all columns have numeric data types.\")\n",
    "\n",
    "# Convert target variable to integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming num_classes is the number of unique classes in your 'emotion_class' column\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "\n",
    "def create_dense_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Assuming X_train and X_test are DataFrames or NumPy arrays\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Create the model\n",
    "dense_model = create_dense_model(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n",
    "dense_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_prob = dense_model.predict(X_test)\n",
    "y_pred = y_pred_prob.argmax(axis=-1)  # Get the index of the maximum value along the last axis\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283dc490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_18700\\3729157061.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not all(X.applymap(lambda x: isinstance(x, (int, float))).all()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.2656 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.23125\n",
      "Confusion Matrix:\n",
      "[[37  0  0  0]\n",
      " [44  0  0  0]\n",
      " [45  0  0  0]\n",
      " [34  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Extract features (X) and target values (y)\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Ensure all data types are numeric\n",
    "if not all(X.applymap(lambda x: isinstance(x, (int, float))).all()):\n",
    "    raise ValueError(\"Not all columns have numeric data types.\")\n",
    "\n",
    "# Convert target variable to integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming num_classes is the number of unique classes in your 'emotion_class' column\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Create a simple Dense neural network model\n",
    "def create_dense_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))  # Corrected input_shape\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Assuming X_train and X_test are DataFrames or NumPy arrays\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Create the model\n",
    "dense_model = create_dense_model(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n",
    "dense_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_prob = dense_model.predict(X_test)\n",
    "y_pred = y_pred_prob.argmax(axis=-1)  # Get the index of the maximum value along the last axis\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a00e015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_18700\\1060431498.py:17: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not all(X.applymap(lambda x: isinstance(x, (int, float))).all()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 17ms/step - loss: nan - accuracy: 0.2656 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 2/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 3/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 4/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 5/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 6/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 7/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 8/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 9/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 10/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 11/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 12/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 13/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 14/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 15/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 16/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 17/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 18/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 19/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 20/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 21/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 22/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 23/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 24/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 25/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 26/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 27/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 28/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 29/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 30/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 31/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 32/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 33/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 34/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 35/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 36/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 37/40\n",
      "20/20 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 38/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 39/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "Epoch 40/40\n",
      "20/20 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.2672 - val_loss: nan - val_accuracy: 0.2313\n",
      "5/5 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.2313\n",
      "Test Accuracy: 23.13%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Ensure all columns have numeric data types\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "if not all(X.applymap(lambda x: isinstance(x, (int, float))).all()):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    X = X.apply(lambda x: label_encoder.fit_transform(x) if x.dtype == object else x)\n",
    "\n",
    "# Convert target variable to integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(concatenated_df['emotion_class'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the labels are one-hot encoded for categorical crossentropy\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Reshape((input_shape, 1), input_shape=(input_shape,)))\n",
    "    model.add(layers.Conv1D(256, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Get input shape\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Set the batch size\n",
    "your_batch_size = 32  \n",
    "\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "cnn_model.fit(X_train, y_train_categorical, epochs=40, validation_data=(X_test, y_test_categorical), batch_size=your_batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test_categorical)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b95dc6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - X_train_reshaped: (100, 128, 1) y_train: 640\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes - X_train_reshaped:\", X_train_reshaped.shape, \"y_train:\", len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e893f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (-1, X_train.shape[1], 1)  \n",
    "\n",
    "X_train_reshaped = X_train.values.reshape(input_shape)\n",
    "X_test_reshaped = X_test.values.reshape(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "868e197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 9ms/step - loss: nan - accuracy: 0.2688\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.2672\n",
      "5/5 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.2313\n",
      "Test Accuracy: 23.13%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data for the model\n",
    "input_shape = (-1, X_train.shape[1], 1)  # Assuming the input size is (number_of_features, 1)\n",
    "X_train_reshaped = X_train.values.reshape(input_shape)\n",
    "X_test_reshaped = X_test.values.reshape(input_shape)\n",
    "\n",
    "# Check if the number of samples match between X_train_reshaped and y_train\n",
    "if X_train_reshaped.shape[0] != len(y_train):\n",
    "    print(f\"Shapes - X_train_reshaped: {X_train_reshaped.shape}, y_train: {len(y_train)}\")\n",
    "    raise ValueError(\"Number of samples in X_train_reshaped and y_train do not match.\")\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add layers based on the provided architecture\n",
    "model.add(layers.Conv1D(16, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "model.add(layers.Conv1D(32, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "model.add(layers.LSTM(128, activation='tanh', recurrent_activation='sigmoid'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(np.max(y_encoded) + 1, activation='softmax'))  # Adjust output size based on the number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test Accuracy: {accuracy[1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58224377",
   "metadata": {},
   "source": [
    "pip install numpy pandas scikit-learn keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19573ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f482a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3884 - accuracy: 0.2486\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2097\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3872 - accuracy: 0.2403\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2472\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3867 - accuracy: 0.2431\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2653\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2667\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3862 - accuracy: 0.2708\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3855 - accuracy: 0.2667\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3881 - accuracy: 0.2528\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.3856 - accuracy: 0.3000\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3885 - accuracy: 0.2667\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3896 - accuracy: 0.2403\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2389\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3860 - accuracy: 0.2542\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3879 - accuracy: 0.2458\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2292\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2417\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2472\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3847 - accuracy: 0.3083\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2597\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3877 - accuracy: 0.2375\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3929 - accuracy: 0.2111\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3851 - accuracy: 0.2667\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3891 - accuracy: 0.2556\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3894 - accuracy: 0.2167\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3888 - accuracy: 0.2472\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3872 - accuracy: 0.2667\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3894 - accuracy: 0.2403\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3862 - accuracy: 0.2750\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2556\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2486\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3856 - accuracy: 0.2250\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 13ms/step - loss: 1.3888 - accuracy: 0.2667\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2653\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3899 - accuracy: 0.2222\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3874 - accuracy: 0.2389\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3861 - accuracy: 0.2556\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3892 - accuracy: 0.2222\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3858 - accuracy: 0.2722\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3878 - accuracy: 0.2458\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3862 - accuracy: 0.2542\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3847 - accuracy: 0.2778\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3877 - accuracy: 0.2250\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3904 - accuracy: 0.2583\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3897 - accuracy: 0.2514\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2667\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2569\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.2486\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3890 - accuracy: 0.2514\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3864 - accuracy: 0.2389\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2458\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2556\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2486\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021F33327240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3867 - accuracy: 0.2375\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3908 - accuracy: 0.2458\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3904 - accuracy: 0.2486\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2583\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3885 - accuracy: 0.2056\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3865 - accuracy: 0.2528\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3866 - accuracy: 0.2361\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3877 - accuracy: 0.2264\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3855 - accuracy: 0.2681\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3883 - accuracy: 0.2542\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3864 - accuracy: 0.2431\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021F37A2A7A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.3875 - accuracy: 0.2250\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3917 - accuracy: 0.2208\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3882 - accuracy: 0.2611\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3873 - accuracy: 0.2486\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3870 - accuracy: 0.2306\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3854 - accuracy: 0.2694\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3890 - accuracy: 0.2389\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3867 - accuracy: 0.2528\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3870 - accuracy: 0.2236\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3877 - accuracy: 0.2431\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3872 - accuracy: 0.2681\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.3857 - accuracy: 0.2750\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3912 - accuracy: 0.2347\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3881 - accuracy: 0.2625\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3890 - accuracy: 0.2639\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3909 - accuracy: 0.2639\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3877 - accuracy: 0.2278\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3857 - accuracy: 0.2514\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3871 - accuracy: 0.2417\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2514\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3880 - accuracy: 0.2306\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3890 - accuracy: 0.2389\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3855 - accuracy: 0.2625\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3877 - accuracy: 0.2569\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2375\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3900 - accuracy: 0.2431\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3887 - accuracy: 0.2333\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3881 - accuracy: 0.2611\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2375\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3896 - accuracy: 0.2347\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2319\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3874 - accuracy: 0.2250\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3870 - accuracy: 0.2639\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.3857 - accuracy: 0.2250\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3890 - accuracy: 0.2708\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2611\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3872 - accuracy: 0.2472\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2639\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3878 - accuracy: 0.2431\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3897 - accuracy: 0.2528\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2361\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3883 - accuracy: 0.2444\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3886 - accuracy: 0.2403\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3874 - accuracy: 0.2444\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3861 - accuracy: 0.2125\n",
      "Average Test Accuracy: 24.25%\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3880 - accuracy: 0.2375\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3891 - accuracy: 0.2458\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3875 - accuracy: 0.2611\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3864 - accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3892 - accuracy: 0.2264\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3887 - accuracy: 0.2417\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3857 - accuracy: 0.2556\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3903 - accuracy: 0.2458\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3867 - accuracy: 0.2528\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3843 - accuracy: 0.2653\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.3862 - accuracy: 0.2500\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3905 - accuracy: 0.2139\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3879 - accuracy: 0.2472\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3882 - accuracy: 0.2292\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3887 - accuracy: 0.2278\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3862 - accuracy: 0.2750\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3858 - accuracy: 0.2528\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3885 - accuracy: 0.2708\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3871 - accuracy: 0.2319\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3868 - accuracy: 0.2556\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3873 - accuracy: 0.2444\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3857 - accuracy: 0.2875\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 14ms/step - loss: 1.3910 - accuracy: 0.2236\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3897 - accuracy: 0.2514\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3877 - accuracy: 0.2361\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3863 - accuracy: 0.2361\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3850 - accuracy: 0.2639\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2431\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3857 - accuracy: 0.2806\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2542\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3877 - accuracy: 0.2597\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2319\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3873 - accuracy: 0.2250\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3892 - accuracy: 0.2361\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2250\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3886 - accuracy: 0.2375\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3850 - accuracy: 0.2806\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3892 - accuracy: 0.2611\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3881 - accuracy: 0.2292\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2569\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3856 - accuracy: 0.2361\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3878 - accuracy: 0.2264\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.2472\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3868 - accuracy: 0.2500\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3924 - accuracy: 0.2222\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3887 - accuracy: 0.2361\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3874 - accuracy: 0.2208\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3832 - accuracy: 0.2889\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2625\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3907 - accuracy: 0.2236\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3877 - accuracy: 0.2444\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3876 - accuracy: 0.2528\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2569\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3866 - accuracy: 0.2319\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.3879 - accuracy: 0.2250\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3933 - accuracy: 0.2306\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3886 - accuracy: 0.2250\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3889 - accuracy: 0.2681\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3890 - accuracy: 0.2306\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3873 - accuracy: 0.2611\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3875 - accuracy: 0.2458\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3872 - accuracy: 0.2528\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3875 - accuracy: 0.2375\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3877 - accuracy: 0.2431\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3860 - accuracy: 0.2625\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3864 - accuracy: 0.2375\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3901 - accuracy: 0.2375\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3866 - accuracy: 0.2431\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3883 - accuracy: 0.2292\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3899 - accuracy: 0.2181\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2389\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3886 - accuracy: 0.2306\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2458\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3878 - accuracy: 0.2458\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3865 - accuracy: 0.2639\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3866 - accuracy: 0.2514\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.3860 - accuracy: 0.2750\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3886 - accuracy: 0.2361\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3893 - accuracy: 0.2347\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3890 - accuracy: 0.2375\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3885 - accuracy: 0.2250\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3864 - accuracy: 0.2472\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3906 - accuracy: 0.2500\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2333\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2431\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3889 - accuracy: 0.2083\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3874 - accuracy: 0.2500\n",
      "3/3 [==============================] - 1s 8ms/step - loss: 1.3848 - accuracy: 0.2875\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3942 - accuracy: 0.2292\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3860 - accuracy: 0.2597\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3912 - accuracy: 0.2625\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3876 - accuracy: 0.2736\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3874 - accuracy: 0.2556\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3872 - accuracy: 0.2417\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2597\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2292\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3872 - accuracy: 0.2236\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3882 - accuracy: 0.2292\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.3861 - accuracy: 0.2625\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3891 - accuracy: 0.2514\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3917 - accuracy: 0.2403\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3888 - accuracy: 0.2486\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2278\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3866 - accuracy: 0.2444\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3901 - accuracy: 0.2250\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3882 - accuracy: 0.2778\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3863 - accuracy: 0.2514\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3885 - accuracy: 0.2528\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3872 - accuracy: 0.2667\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.3862 - accuracy: 0.3000\n",
      "Average Test Accuracy: 26.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Initialize 10-fold cross-validation\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over folds\n",
    "for train_index, test_index in skf.split(X, y_encoded):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Reshape input data for the model\n",
    "    input_shape = (-1, X_train.shape[1], 1)\n",
    "    X_train_reshaped = X_train.values.reshape(input_shape)\n",
    "    X_test_reshaped = X_test.values.reshape(input_shape)\n",
    "\n",
    "    # Build the model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(16, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "    model.add(layers.Conv1D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "    model.add(layers.LSTM(128, activation='tanh', recurrent_activation='sigmoid'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(np.max(y_encoded) + 1, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = model.evaluate(X_test_reshaped, y_test)[1]\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Display the average accuracy over all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average Test Accuracy: {average_accuracy * 100:.2f}%\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Initialize 10-fold cross-validation\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over folds\n",
    "for train_index, test_index in skf.split(X, y_encoded):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Reshape input data for the model\n",
    "    input_shape = (-1, X_train.shape[1], 1)\n",
    "    X_train_reshaped = X_train.values.reshape(input_shape)\n",
    "    X_test_reshaped = X_test.values.reshape(input_shape)\n",
    "\n",
    "    # Build the model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(16, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "    model.add(layers.Conv1D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "    model.add(layers.LSTM(128, activation='tanh', recurrent_activation='sigmoid'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(np.max(y_encoded) + 1, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = model.evaluate(X_test_reshaped, y_test)[1]\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Display the average accuracy over all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average Test Accuracy: {average_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d658e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3913 - accuracy: 0.2361\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3874 - accuracy: 0.2486\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3880 - accuracy: 0.2333\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3888 - accuracy: 0.2250\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3875 - accuracy: 0.2403\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3876 - accuracy: 0.2153\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3848 - accuracy: 0.2583\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3895 - accuracy: 0.2097\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3859 - accuracy: 0.2569\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3869 - accuracy: 0.2514\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3886 - accuracy: 0.2542\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3854 - accuracy: 0.2556\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3895 - accuracy: 0.2431\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3885 - accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3883 - accuracy: 0.2347\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3878 - accuracy: 0.2431\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3871 - accuracy: 0.2778\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3876 - accuracy: 0.2250\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3861 - accuracy: 0.2681\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2375\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3911 - accuracy: 0.2306\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2556\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2819\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2389\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2625\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2069\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3862 - accuracy: 0.2417\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.2514\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3878 - accuracy: 0.2306\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3892 - accuracy: 0.2167\n",
      "3/3 [==============================] - 1s 6ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3878 - accuracy: 0.2472\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2472\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3893 - accuracy: 0.2208\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2375\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3877 - accuracy: 0.2375\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3875 - accuracy: 0.2500\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3881 - accuracy: 0.2569\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2597\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2569\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3858 - accuracy: 0.2625\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 13ms/step - loss: 1.3880 - accuracy: 0.2681\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3876 - accuracy: 0.2542\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3860 - accuracy: 0.2472\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3910 - accuracy: 0.2306\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2431\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2597\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2333\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3862 - accuracy: 0.2778\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2431\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3861 - accuracy: 0.2486\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 13ms/step - loss: 1.3900 - accuracy: 0.2139\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3891 - accuracy: 0.2403\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.2514\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3891 - accuracy: 0.2181\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3856 - accuracy: 0.2778\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3863 - accuracy: 0.2528\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3871 - accuracy: 0.2431\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2444\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.2306\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.2222\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3921 - accuracy: 0.2611\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2236\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3864 - accuracy: 0.2667\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3910 - accuracy: 0.2417\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2194\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2444\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2556\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3872 - accuracy: 0.2222\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3876 - accuracy: 0.2278\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3874 - accuracy: 0.2361\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3876 - accuracy: 0.2708\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3895 - accuracy: 0.2458\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2403\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3858 - accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3882 - accuracy: 0.2625\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3915 - accuracy: 0.2375\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2306\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3894 - accuracy: 0.2181\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3870 - accuracy: 0.2361\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3877 - accuracy: 0.2472\n",
      "3/3 [==============================] - 0s 10ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 13ms/step - loss: 1.3891 - accuracy: 0.2194\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3894 - accuracy: 0.2292\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3875 - accuracy: 0.2750\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3861 - accuracy: 0.2694\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3870 - accuracy: 0.2694\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2556\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3892 - accuracy: 0.2236\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3883 - accuracy: 0.2389\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3877 - accuracy: 0.2264\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3876 - accuracy: 0.2500\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 14ms/step - loss: 1.3927 - accuracy: 0.2333\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3930 - accuracy: 0.2306\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3875 - accuracy: 0.2542\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3877 - accuracy: 0.2347\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3897 - accuracy: 0.2431\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2278\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3867 - accuracy: 0.2569\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3873 - accuracy: 0.2403\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3868 - accuracy: 0.2500\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3880 - accuracy: 0.2278\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "Ensemble Test Accuracy: 26.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Initialize 10-fold cross-validation\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "ensemble_predictions = []\n",
    "\n",
    "# Iterate over folds\n",
    "for train_index, test_index in skf.split(X, y_encoded):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Reshape input data for the model\n",
    "    input_shape = (-1, X_train.shape[1], 1)\n",
    "    X_train_reshaped = X_train.values.reshape(input_shape)\n",
    "    X_test_reshaped = X_test.values.reshape(input_shape)\n",
    "\n",
    "    # Build the model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(16, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "    model.add(layers.Conv1D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=1))\n",
    "    model.add(layers.LSTM(128, activation='tanh', recurrent_activation='sigmoid'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(np.max(y_encoded) + 1, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "    # Evaluate the model and store predictions\n",
    "    predictions = model.predict(X_test_reshaped)\n",
    "    ensemble_predictions.append(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Stack predictions from all folds\n",
    "ensemble_predicted_classes = np.hstack(ensemble_predictions)\n",
    "\n",
    "# Convert ensemble_predicted_classes to class labels\n",
    "ensemble_predicted_labels = label_encoder.inverse_transform(ensemble_predicted_classes)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "ensemble_accuracy = np.sum(ensemble_predicted_labels == y_text) / len(y_text)\n",
    "print(f\"Ensemble Test Accuracy: {ensemble_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb61fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Test Accuracy: 24.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "boosting_predictions = []\n",
    "\n",
    "# Iterate over folds\n",
    "for train_index, test_index in skf.split(X, y_encoded):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Build and train the XGBoost model\n",
    "    model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "    model.fit(X_train, y_train, sample_weight=np.array([class_weights[i] for i in y_train]))\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    boosting_predictions.append(predictions)\n",
    "\n",
    "# Stack predictions from all folds\n",
    "boosting_predicted_classes = np.hstack(boosting_predictions)\n",
    "\n",
    "# Convert boosting_predicted_classes to class labels\n",
    "boosting_predicted_labels = label_encoder.inverse_transform(boosting_predicted_classes)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "boosting_accuracy = accuracy_score(y_text, boosting_predicted_labels)\n",
    "print(f\"Boosting Test Accuracy: {boosting_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a08ad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 50.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Make predictions oen the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bc40ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 40.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f4242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 64.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = Amigosn_with_arousal['arousal_category']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Make predictions oen the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0be57dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 64.62%\n",
      "Model saved to modela.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = Amigosn_with_arousal['arousal_category']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Save the best model to a file\n",
    "model_filename = ' '\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f80711f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 99.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = Amigosn_with_valence['valence_category']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Make predictions oen the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f869441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 99.62%\n",
      "Model saved to best_xgboost_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = Amigosn_with_valence['valence_category']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Save the best model to a file\n",
    "model_filename = 'best_xgboost_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d4a35be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 55.62%\n",
      "Model saved to best_xgboost_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_text_train, y_text_test = train_test_split(\n",
    "    concatenated_df.drop('emotion_class', axis=1),\n",
    "    Amigosn_with_valence['valence_category'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Amigosn_with_valence['valence_category']\n",
    ")\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_text_train)\n",
    "y_test_encoded = label_encoder.transform(y_text_test)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "best_model.fit(X_train, y_train_encoded, sample_weight=np.array([class_weights[i] for i in y_train_encoded]))\n",
    "\n",
    "# Save the best model to a file\n",
    "model_filename = 'best_xgboost_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Plot learning curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(best_model, X_train, y_train_encoded, cv=5, scoring='accuracy')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Accuracy')\n",
    "plt.plot(train_sizes, np.mean(valid_scores, axis=1), label='Validation Accuracy')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text_test, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb254f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "beac840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.62%\n",
      "Cross-Validation Mean Accuracy: 50.50%\n",
      "Cross-Validation Accuracy Standard Deviation: 3.94%\n",
      "The model shows no strong evidence of overfitting.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = Amigosn_with_valence['valence_category']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = cross_val_score(best_model, X, y_encoded, cv=k_fold, scoring='accuracy')\n",
    "\n",
    "# Calculate mean and standard deviation of cross-validation results\n",
    "mean_cv_accuracy = np.mean(cv_results)\n",
    "std_cv_accuracy = np.std(cv_results)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_predictions = best_model.predict(X)\n",
    "train_accuracy = accuracy_score(y_encoded, train_predictions)\n",
    "\n",
    "# Print accuracy on both sets\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Cross-Validation Mean Accuracy: {mean_cv_accuracy * 100:.2f}%\")\n",
    "print(f\"Cross-Validation Accuracy Standard Deviation: {std_cv_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Check for overfitting based on the difference between training and cross-validation accuracies\n",
    "overfitting_threshold = 2.0  # Set your own threshold\n",
    "overfitting_indicator = train_accuracy - mean_cv_accuracy\n",
    "\n",
    "if overfitting_indicator > overfitting_threshold:\n",
    "    print(\"Warning: The model may be overfitting to the training data.\")\n",
    "else:\n",
    "    print(\"The model shows no strong evidence of overfitting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "070c3536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n",
      "Validation Accuracy: 46.88%\n",
      "The model shows no strong evidence of overfitting.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(concatenated_df.drop('emotion_class', axis=1),\n",
    "                                                    Amigosn_with_valence['valence_category'],\n",
    "                                                    test_size=0.4, random_state=42, stratify=Amigosn_with_valence['valence_category'])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights_train = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "best_model.fit(X_train, y_train_encoded, sample_weight=np.array([class_weights_train[i] for i in y_train_encoded]))\n",
    "\n",
    "# Plot learning curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(best_model, X_train, y_train_encoded, cv=5, scoring='accuracy')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Accuracy')\n",
    "plt.plot(train_sizes, np.mean(valid_scores, axis=1), label='Validation Accuracy')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_predictions = best_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train_encoded, train_predictions)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_predictions = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val_encoded, val_predictions)\n",
    "\n",
    "# Print accuracy on both sets\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Check for overfitting based on the difference between training and validation accuracies\n",
    "overfitting_threshold = 2.0  \n",
    "overfitting_indicator = train_accuracy - val_accuracy\n",
    "\n",
    "if overfitting_indicator > overfitting_threshold:\n",
    "    print(\"Warning: The model may be overfitting to the training data.\")\n",
    "else:\n",
    "    print(\"The model shows no strong evidence of overfitting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32cc8f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 55.62%\n",
      "Model saved to modelv.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_text_train, y_text_test = train_test_split(\n",
    "    concatenated_df.drop('emotion_class', axis=1),\n",
    "    Amigosn_with_valence['valence_category'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Amigosn_with_valence['valence_category']\n",
    ")\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_text_train)\n",
    "y_test_encoded = label_encoder.transform(y_text_test)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "best_model.fit(X_train, y_train_encoded, sample_weight=np.array([class_weights[i] for i in y_train_encoded]))\n",
    "\n",
    "# Save the best model to a file\n",
    "model_filename = 'modelv.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text_test, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2feb5f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelA Accuracy: 0.20625\n",
      "Best XGBoost Model Accuracy: 0.2625\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the models\n",
    "modela = joblib.load('modela.joblib')\n",
    "best_xgboost_model = joblib.load('best_xgboost_model.joblib')\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Make predictions with both models\n",
    "preds_modela = modela.predict(X_test)\n",
    "preds_xgboost = best_xgboost_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for each model\n",
    "accuracy_modela = accuracy_score(y_test, preds_modela)\n",
    "accuracy_xgboost = accuracy_score(y_test, preds_xgboost)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"ModelA Accuracy:\", accuracy_modela)\n",
    "print(\"Best XGBoost Model Accuracy:\", accuracy_xgboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2f6368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 1s 18ms/step - loss: 0.7011 - accuracy: 0.5078 - val_loss: 0.6934 - val_accuracy: 0.5391\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.4863 - val_loss: 0.6944 - val_accuracy: 0.5156\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.4844 - val_loss: 0.7021 - val_accuracy: 0.4609\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6886 - accuracy: 0.5645 - val_loss: 0.6979 - val_accuracy: 0.4375\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6869 - accuracy: 0.5625 - val_loss: 0.6994 - val_accuracy: 0.4219\n",
      "20/20 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "Best XGBoost Model Test Accuracy: 51.25%\n",
      "XGBoost Model saved to model_with_cnn_and xgboost.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_text_train, y_text_test = train_test_split(\n",
    "    concatenated_df.drop('emotion_class', axis=1),\n",
    "    Amigosn_with_valence['t'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Amigosn_with_valence['valence_category']\n",
    ")\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_text_train)\n",
    "y_test_encoded = label_encoder.transform(y_text_test)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# 1D CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for multiclass\n",
    "\n",
    "# Compile the CNN model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data for the CNN\n",
    "X_train_cnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Train the CNN model\n",
    "model_cnn.fit(X_train_cnn, y_train_encoded, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Extract features from the CNN model\n",
    "cnn_features_train = model_cnn.predict(X_train_cnn)\n",
    "cnn_features_test = model_cnn.predict(X_test_cnn)\n",
    "\n",
    "# Combine CNN features with existing features\n",
    "X_train_combined = np.hstack([X_train.values, cnn_features_train])\n",
    "X_test_combined = np.hstack([X_test.values, cnn_features_test])\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for XGBoost\n",
    "param_grid_xgboost = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "model_xgboost = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "grid_search_xgboost = GridSearchCV(model_xgboost, param_grid_xgboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_xgboost.fit(X_train_combined, y_train_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params_xgboost = grid_search_xgboost.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model_xgboost = xgb.XGBClassifier(**best_params_xgboost, objective='multi:softmax', num_class=np.max(y_train_encoded) + 1, random_state=42)\n",
    "best_model_xgboost.fit(X_train_combined, y_train_encoded, sample_weight=np.array([class_weights[i] for i in y_train_encoded]))\n",
    "\n",
    "# Save the best XGBoost model to a file\n",
    "model_filename_xgboost = 'model_with_cnn_and xgboost.joblib'\n",
    "joblib.dump(best_model_xgboost, model_filename_xgboost)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions_xgboost = best_model_xgboost.predict(X_test_combined)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels_xgboost = label_encoder.inverse_transform(best_predictions_xgboost)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy_xgboost = accuracy_score(y_text_test, best_predicted_labels_xgboost)\n",
    "print(f\"Best XGBoost Model Test Accuracy: {best_accuracy_xgboost * 100:.2f}%\")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"XGBoost Model saved to {model_filename_xgboost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5352560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 1s 20ms/step - loss: 0.6914 - accuracy: 0.5352 - val_loss: 0.6832 - val_accuracy: 0.5859\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5273 - val_loss: 0.6845 - val_accuracy: 0.5859\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.5742 - val_loss: 0.6953 - val_accuracy: 0.5234\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6760 - accuracy: 0.5645 - val_loss: 0.6923 - val_accuracy: 0.5469\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6695 - accuracy: 0.6309 - val_loss: 0.6942 - val_accuracy: 0.5312\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Best XGBoost Model Test Accuracy: 48.12%\n",
      "XGBoost Model saved to model_gcboost_with_arousal_and_cnn.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = Amigosn_with_arousal['arousal_category']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# 1D CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for multiclass\n",
    "\n",
    "# Compile the CNN model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data for the CNN\n",
    "X_train_cnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Train the CNN model\n",
    "model_cnn.fit(X_train_cnn, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Extract features from the CNN model\n",
    "cnn_features_train = model_cnn.predict(X_train_cnn)\n",
    "cnn_features_test = model_cnn.predict(X_test_cnn)\n",
    "\n",
    "# Combine CNN features with existing features\n",
    "X_train_combined = np.hstack([X_train.values, cnn_features_train])\n",
    "X_test_combined = np.hstack([X_test.values, cnn_features_test])\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for XGBoost\n",
    "param_grid_xgboost = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "\n",
    "model_xgboost = xgb.XGBClassifier(objective='binary:logistic', random_state=42)  # Adjust objective for multiclass\n",
    "grid_search_xgboost = GridSearchCV(model_xgboost, param_grid_xgboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_xgboost.fit(X_train_combined, y_train)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params_xgboost = grid_search_xgboost.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model_xgboost = xgb.XGBClassifier(**best_params_xgboost, objective='binary:logistic', random_state=42)  # Adjust objective for multiclass\n",
    "best_model_xgboost.fit(X_train_combined, y_train, sample_weight=np.array([class_weights[i] for i in y_train]))\n",
    "\n",
    "# Save the best XGBoost model to a file\n",
    "model_filename_xgboost = 'model_gcboost_with_arousal_and_cnn.joblib'\n",
    "joblib.dump(best_model_xgboost, model_filename_xgboost)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions_xgboost = best_model_xgboost.predict(X_test_combined)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels_xgboost = label_encoder.inverse_transform(best_predictions_xgboost)\n",
    "\n",
    "# Convert string labels to numeric labels in y_pred\n",
    "label_mapping = {'HA': 1, 'LA': 0}\n",
    "numeric_labels_y_pred = np.array([label_mapping[label] for label in best_predicted_labels_xgboost])\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy_xgboost = accuracy_score(y_test, numeric_labels_y_pred)\n",
    "print(f\"Best XGBoost Model Test Accuracy: {best_accuracy_xgboost * 100:.2f}%\")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"XGBoost Model saved to {model_filename_xgboost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9c574b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 100.00%\n",
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Define a list of dictionaries with different sets of hyperparameters\n",
    "param_combinations = [\n",
    "    {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0},\n",
    "    {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 1},\n",
    "    {'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 5}\n",
    "]\n",
    "\n",
    "# Store results\n",
    "best_accuracy = 0.0\n",
    "best_params = None\n",
    "\n",
    "# Iterate over different parameter combinations\n",
    "for params in param_combinations:\n",
    "    # Build the XGBoost model with the current hyperparameters\n",
    "    model = xgb.XGBClassifier(**params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "    model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Calculate accuracy for the current set of hyperparameters\n",
    "    current_accuracy = accuracy_score(y_text, predicted_labels)\n",
    "\n",
    "    # Update best accuracy and parameters if the current set is better\n",
    "    if current_accuracy > best_accuracy:\n",
    "        best_accuracy = current_accuracy\n",
    "        best_params = params\n",
    "\n",
    "# Display the best results\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3faf7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Amigos_with_three_classes = Amigosn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fb8ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arousal_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  arousal_category\n",
       "0               LA\n",
       "1               LA\n",
       "2               LA\n",
       "3               LA\n",
       "4               HA"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amigosn_with_arousal = pd.DataFrame()\n",
    "Amigosn_with_arousal['arousal_category'] = np.where(Amigosn['arousal'] > 5, 'HA', 'LA')\n",
    "Amigosn_with_arousal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d1d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09dc3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arousal</th>\n",
       "      <th>valence</th>\n",
       "      <th>GSR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>800.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>748.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.152208</td>\n",
       "      <td>5.165602</td>\n",
       "      <td>40168.552012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.906878</td>\n",
       "      <td>2.285636</td>\n",
       "      <td>47740.161852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2899.501846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.549152</td>\n",
       "      <td>3.320816</td>\n",
       "      <td>13271.101323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.560000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22049.058117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.586440</td>\n",
       "      <td>6.979520</td>\n",
       "      <td>41759.322732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.160000</td>\n",
       "      <td>295549.604403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          arousal     valence            GSR\n",
       "count  800.000000  800.000000     748.000000\n",
       "mean     5.152208    5.165602   40168.552012\n",
       "std      1.906878    2.285636   47740.161852\n",
       "min      0.000000    0.000000    2899.501846\n",
       "25%      3.549152    3.320816   13271.101323\n",
       "50%      5.560000    5.000000   22049.058117\n",
       "75%      6.586440    6.979520   41759.322732\n",
       "max      9.000000    9.160000  295549.604403"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amigos_with_three_class = Amigosn\n",
    "Amigos_with_three_class.head()\n",
    "Amigos_with_three_class.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61fb0a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arousal</th>\n",
       "      <th>valence</th>\n",
       "      <th>GSR</th>\n",
       "      <th>emotion_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.938568</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22080.029612</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.877136</td>\n",
       "      <td>6.542664</td>\n",
       "      <td>24385.023872</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.238904</td>\n",
       "      <td>21047.319425</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.965872</td>\n",
       "      <td>8.590440</td>\n",
       "      <td>15939.235580</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.436864</td>\n",
       "      <td>27571.004599</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    arousal   valence           GSR emotion_class\n",
       "0  2.938568  5.000000  22080.029612          LALV\n",
       "1  4.877136  6.542664  24385.023872          LAHV\n",
       "2  5.000000  3.238904  21047.319425          LALV\n",
       "3  2.965872  8.590440  15939.235580          LAHV\n",
       "5  5.000000  1.436864  27571.004599          LALV"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvla_rows = Amigos_with_three_class[Amigos_with_three_class['emotion_class'] == 'HALV'].index.tolist()\n",
    "\n",
    "# Drop rows with HVLA and rows with the same index\n",
    "new_amigos = Amigos_with_three_class.drop(hvla_rows)\n",
    "new_amigos = Amigos_with_three_class.drop(hvla_rows, axis=0)\n",
    "new_amigos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65ad7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column 'emotion_class' does not contain 'HVLA'.\n"
     ]
    }
   ],
   "source": [
    "if 'HALV' in new_amigos['emotion_class'].values:\n",
    "    print(f\"The column '{'emotion_class'}' contains 'HVLA'.\")\n",
    "else:\n",
    "    print(f\"The column '{'emotion_class'}' does not contain 'HVLA'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89d28079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(575,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "new_amigos.fillna(0, inplace=True)\n",
    "\n",
    "# Assuming 'GSR' is a column in the DataFrame Amigos0\n",
    "shape_of_gsr_column = new_amigos['GSR'].shape\n",
    "\n",
    "print(shape_of_gsr_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd4b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the split DataFrame: (14, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window_1</th>\n",
       "      <th>Window_2</th>\n",
       "      <th>Window_3</th>\n",
       "      <th>Window_4</th>\n",
       "      <th>Window_5</th>\n",
       "      <th>Window_6</th>\n",
       "      <th>Window_7</th>\n",
       "      <th>Window_8</th>\n",
       "      <th>Window_9</th>\n",
       "      <th>Window_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Window_31</th>\n",
       "      <th>Window_32</th>\n",
       "      <th>Window_33</th>\n",
       "      <th>Window_34</th>\n",
       "      <th>Window_35</th>\n",
       "      <th>Window_36</th>\n",
       "      <th>Window_37</th>\n",
       "      <th>Window_38</th>\n",
       "      <th>Window_39</th>\n",
       "      <th>Window_40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22080.029612</td>\n",
       "      <td>198178.600760</td>\n",
       "      <td>30121.270924</td>\n",
       "      <td>18749.789104</td>\n",
       "      <td>16219.662829</td>\n",
       "      <td>146069.449829</td>\n",
       "      <td>22917.107839</td>\n",
       "      <td>73033.596057</td>\n",
       "      <td>13171.258802</td>\n",
       "      <td>22596.167687</td>\n",
       "      <td>...</td>\n",
       "      <td>80523.773850</td>\n",
       "      <td>30042.105345</td>\n",
       "      <td>23227.230812</td>\n",
       "      <td>13803.791373</td>\n",
       "      <td>15474.333256</td>\n",
       "      <td>30227.825781</td>\n",
       "      <td>64699.591537</td>\n",
       "      <td>17733.458872</td>\n",
       "      <td>8581.887428</td>\n",
       "      <td>4138.197649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24385.023872</td>\n",
       "      <td>91835.492737</td>\n",
       "      <td>50465.688381</td>\n",
       "      <td>8569.148604</td>\n",
       "      <td>15026.268915</td>\n",
       "      <td>153599.142277</td>\n",
       "      <td>38941.432758</td>\n",
       "      <td>71243.549518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24918.359894</td>\n",
       "      <td>...</td>\n",
       "      <td>73787.220584</td>\n",
       "      <td>45371.326815</td>\n",
       "      <td>35606.459614</td>\n",
       "      <td>15098.791653</td>\n",
       "      <td>15202.631048</td>\n",
       "      <td>19056.555556</td>\n",
       "      <td>41841.919351</td>\n",
       "      <td>24151.145512</td>\n",
       "      <td>10007.794228</td>\n",
       "      <td>4042.324020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21047.319425</td>\n",
       "      <td>77801.766853</td>\n",
       "      <td>38841.986674</td>\n",
       "      <td>2899.501846</td>\n",
       "      <td>19245.430751</td>\n",
       "      <td>150255.105200</td>\n",
       "      <td>34655.327135</td>\n",
       "      <td>68427.446278</td>\n",
       "      <td>14783.867200</td>\n",
       "      <td>22485.679160</td>\n",
       "      <td>...</td>\n",
       "      <td>58992.256148</td>\n",
       "      <td>39007.726069</td>\n",
       "      <td>23315.141116</td>\n",
       "      <td>16397.180817</td>\n",
       "      <td>15411.556829</td>\n",
       "      <td>20520.933868</td>\n",
       "      <td>53471.289925</td>\n",
       "      <td>17938.503872</td>\n",
       "      <td>12400.013021</td>\n",
       "      <td>4312.030567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15939.235580</td>\n",
       "      <td>92498.537239</td>\n",
       "      <td>18404.726749</td>\n",
       "      <td>7931.859988</td>\n",
       "      <td>54385.251814</td>\n",
       "      <td>82771.743359</td>\n",
       "      <td>35126.822581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23869.938836</td>\n",
       "      <td>...</td>\n",
       "      <td>36883.049956</td>\n",
       "      <td>45506.053409</td>\n",
       "      <td>27704.124943</td>\n",
       "      <td>15478.933592</td>\n",
       "      <td>14585.986159</td>\n",
       "      <td>20355.630878</td>\n",
       "      <td>70150.511446</td>\n",
       "      <td>14248.882606</td>\n",
       "      <td>8817.460346</td>\n",
       "      <td>4716.495515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27571.004599</td>\n",
       "      <td>123992.590463</td>\n",
       "      <td>16817.890878</td>\n",
       "      <td>2955.819364</td>\n",
       "      <td>31562.225342</td>\n",
       "      <td>151906.277258</td>\n",
       "      <td>37395.332465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26886.308719</td>\n",
       "      <td>...</td>\n",
       "      <td>28710.181961</td>\n",
       "      <td>74347.112929</td>\n",
       "      <td>39035.909247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15714.249467</td>\n",
       "      <td>24557.571159</td>\n",
       "      <td>29833.494444</td>\n",
       "      <td>21695.196470</td>\n",
       "      <td>9572.956062</td>\n",
       "      <td>4268.249178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Window_1       Window_2      Window_3      Window_4      Window_5  \\\n",
       "0  22080.029612  198178.600760  30121.270924  18749.789104  16219.662829   \n",
       "1  24385.023872   91835.492737  50465.688381   8569.148604  15026.268915   \n",
       "2  21047.319425   77801.766853  38841.986674   2899.501846  19245.430751   \n",
       "3  15939.235580   92498.537239  18404.726749   7931.859988  54385.251814   \n",
       "4  27571.004599  123992.590463  16817.890878   2955.819364  31562.225342   \n",
       "\n",
       "        Window_6      Window_7      Window_8      Window_9     Window_10  ...  \\\n",
       "0  146069.449829  22917.107839  73033.596057  13171.258802  22596.167687  ...   \n",
       "1  153599.142277  38941.432758  71243.549518      0.000000  24918.359894  ...   \n",
       "2  150255.105200  34655.327135  68427.446278  14783.867200  22485.679160  ...   \n",
       "3   82771.743359  35126.822581      0.000000      0.000000  23869.938836  ...   \n",
       "4  151906.277258  37395.332465      0.000000      0.000000  26886.308719  ...   \n",
       "\n",
       "      Window_31     Window_32     Window_33     Window_34     Window_35  \\\n",
       "0  80523.773850  30042.105345  23227.230812  13803.791373  15474.333256   \n",
       "1  73787.220584  45371.326815  35606.459614  15098.791653  15202.631048   \n",
       "2  58992.256148  39007.726069  23315.141116  16397.180817  15411.556829   \n",
       "3  36883.049956  45506.053409  27704.124943  15478.933592  14585.986159   \n",
       "4  28710.181961  74347.112929  39035.909247      0.000000  15714.249467   \n",
       "\n",
       "      Window_36     Window_37     Window_38     Window_39    Window_40  \n",
       "0  30227.825781  64699.591537  17733.458872   8581.887428  4138.197649  \n",
       "1  19056.555556  41841.919351  24151.145512  10007.794228  4042.324020  \n",
       "2  20520.933868  53471.289925  17938.503872  12400.013021  4312.030567  \n",
       "3  20355.630878  70150.511446  14248.882606   8817.460346  4716.495515  \n",
       "4  24557.571159  29833.494444  21695.196470   9572.956062  4268.249178  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Amigos_new0 = pd.DataFrame({'GSR': np.random.rand(575)})\n",
    "\n",
    "original_data = new_amigos['GSR'].values\n",
    "\n",
    "\n",
    "num_columns = 40\n",
    "column_size = original_data.shape[0] // num_columns\n",
    "\n",
    "\n",
    "total_elements = num_columns * column_size\n",
    "\n",
    "\n",
    "split_data = original_data[:total_elements].reshape((column_size, num_columns), order='F')\n",
    "\n",
    "split_dataframe = pd.DataFrame(split_data, columns=[f'Window_{i+1}' for i in range(num_columns)])\n",
    "\n",
    "print(\"Shape of the split DataFrame:\", split_dataframe.shape)\n",
    "\n",
    "split_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08cc454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the split DataFrame: (40, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window_1</th>\n",
       "      <th>Window_2</th>\n",
       "      <th>Window_3</th>\n",
       "      <th>Window_4</th>\n",
       "      <th>Window_5</th>\n",
       "      <th>Window_6</th>\n",
       "      <th>Window_7</th>\n",
       "      <th>Window_8</th>\n",
       "      <th>Window_9</th>\n",
       "      <th>Window_10</th>\n",
       "      <th>Window_11</th>\n",
       "      <th>Window_12</th>\n",
       "      <th>Window_13</th>\n",
       "      <th>Window_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22080.029612</td>\n",
       "      <td>21307.532478</td>\n",
       "      <td>36061.299155</td>\n",
       "      <td>23897.913856</td>\n",
       "      <td>19294.550337</td>\n",
       "      <td>8494.271861</td>\n",
       "      <td>24999.216663</td>\n",
       "      <td>38887.260665</td>\n",
       "      <td>7459.637819</td>\n",
       "      <td>22778.943814</td>\n",
       "      <td>8530.692901</td>\n",
       "      <td>24114.632085</td>\n",
       "      <td>15714.249467</td>\n",
       "      <td>17938.503872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24385.023872</td>\n",
       "      <td>19767.081042</td>\n",
       "      <td>33736.607923</td>\n",
       "      <td>25830.316767</td>\n",
       "      <td>12968.275356</td>\n",
       "      <td>9184.462114</td>\n",
       "      <td>40127.224066</td>\n",
       "      <td>31540.475070</td>\n",
       "      <td>9902.524014</td>\n",
       "      <td>21592.271382</td>\n",
       "      <td>8519.599590</td>\n",
       "      <td>24746.385405</td>\n",
       "      <td>16674.898451</td>\n",
       "      <td>14248.882606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21047.319425</td>\n",
       "      <td>18749.789104</td>\n",
       "      <td>22910.775650</td>\n",
       "      <td>19535.653774</td>\n",
       "      <td>16263.368808</td>\n",
       "      <td>10517.512783</td>\n",
       "      <td>18884.003076</td>\n",
       "      <td>36929.067712</td>\n",
       "      <td>12428.933241</td>\n",
       "      <td>21687.209437</td>\n",
       "      <td>8226.798209</td>\n",
       "      <td>28837.641967</td>\n",
       "      <td>12617.781115</td>\n",
       "      <td>21695.196470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15939.235580</td>\n",
       "      <td>8569.148604</td>\n",
       "      <td>26727.811820</td>\n",
       "      <td>23612.937825</td>\n",
       "      <td>10055.447586</td>\n",
       "      <td>11337.317740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32859.095530</td>\n",
       "      <td>11303.738230</td>\n",
       "      <td>26720.034562</td>\n",
       "      <td>7038.752072</td>\n",
       "      <td>38700.847863</td>\n",
       "      <td>15589.878058</td>\n",
       "      <td>15122.099687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27571.004599</td>\n",
       "      <td>2899.501846</td>\n",
       "      <td>22917.107839</td>\n",
       "      <td>24866.900934</td>\n",
       "      <td>11843.631573</td>\n",
       "      <td>7446.844936</td>\n",
       "      <td>18143.293540</td>\n",
       "      <td>35745.017041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24256.992266</td>\n",
       "      <td>6775.563126</td>\n",
       "      <td>29983.747214</td>\n",
       "      <td>15039.120230</td>\n",
       "      <td>26636.190642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Window_1      Window_2      Window_3      Window_4      Window_5  \\\n",
       "0  22080.029612  21307.532478  36061.299155  23897.913856  19294.550337   \n",
       "1  24385.023872  19767.081042  33736.607923  25830.316767  12968.275356   \n",
       "2  21047.319425  18749.789104  22910.775650  19535.653774  16263.368808   \n",
       "3  15939.235580   8569.148604  26727.811820  23612.937825  10055.447586   \n",
       "4  27571.004599   2899.501846  22917.107839  24866.900934  11843.631573   \n",
       "\n",
       "       Window_6      Window_7      Window_8      Window_9     Window_10  \\\n",
       "0   8494.271861  24999.216663  38887.260665   7459.637819  22778.943814   \n",
       "1   9184.462114  40127.224066  31540.475070   9902.524014  21592.271382   \n",
       "2  10517.512783  18884.003076  36929.067712  12428.933241  21687.209437   \n",
       "3  11337.317740      0.000000  32859.095530  11303.738230  26720.034562   \n",
       "4   7446.844936  18143.293540  35745.017041      0.000000  24256.992266   \n",
       "\n",
       "     Window_11     Window_12     Window_13     Window_14  \n",
       "0  8530.692901  24114.632085  15714.249467  17938.503872  \n",
       "1  8519.599590  24746.385405  16674.898451  14248.882606  \n",
       "2  8226.798209  28837.641967  12617.781115  21695.196470  \n",
       "3  7038.752072  38700.847863  15589.878058  15122.099687  \n",
       "4  6775.563126  29983.747214  15039.120230  26636.190642  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Amigos_new0 = pd.DataFrame({'GSR': np.random.rand(575)})\n",
    "\n",
    "# Extract the 'GSR' column\n",
    "original_data = new_amigos['GSR'].values\n",
    "\n",
    "# Define the number of columns and the size of each column\n",
    "num_columns = 40\n",
    "column_size = original_data.shape[0] // num_columns\n",
    "\n",
    "# Calculate the number of elements to keep\n",
    "total_elements = num_columns * column_size\n",
    "\n",
    "# Reshape the 'GSR' column into a 2D array with 40 columns and 800 rows\n",
    "split_data2 = original_data[:total_elements].reshape((num_columns, column_size), order='F')\n",
    "\n",
    "# Convert the result back to a DataFrame if needed\n",
    "split_dataframe2 = pd.DataFrame(split_data2, columns=[f'Window_{i+1}' for i in range(column_size)])\n",
    "\n",
    "print(\"Shape of the split DataFrame:\", split_dataframe2.shape)\n",
    "\n",
    "split_dataframe2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34a6978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LALV\n",
      "1      LAHV\n",
      "2      LALV\n",
      "3      LAHV\n",
      "4       NaN\n",
      "       ... \n",
      "794    HAHV\n",
      "795    HAHV\n",
      "797    LAHV\n",
      "798    HAHV\n",
      "799    HAHV\n",
      "Name: emotion_class, Length: 584, dtype: object\n"
     ]
    }
   ],
   "source": [
    "concatenated_df1 = pd.concat([split_dataframe2, new_amigos['emotion_class']], axis=1)\n",
    "\n",
    "concatenated_df1.head()\n",
    "print(concatenated_df1['emotion_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42a1cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each column using shape:\n",
      "(584, 15)\n",
      "\n",
      "Size of each column using len:\n",
      "Window_1         584\n",
      "Window_2         584\n",
      "Window_3         584\n",
      "Window_4         584\n",
      "Window_5         584\n",
      "Window_6         584\n",
      "Window_7         584\n",
      "Window_8         584\n",
      "Window_9         584\n",
      "Window_10        584\n",
      "Window_11        584\n",
      "Window_12        584\n",
      "Window_13        584\n",
      "Window_14        584\n",
      "emotion_class    584\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "column_sizes_shape = concatenated_df.shape\n",
    "print(\"Size of each column using shape:\")\n",
    "print(column_sizes_shape)\n",
    "\n",
    "# Display the size of each column using len\n",
    "column_sizes_len = concatenated_df1.apply(len)\n",
    "print(\"\\nSize of each column using len:\")\n",
    "print(column_sizes_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f07eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Description:\n",
      "            Window_1       Window_2       Window_3      Window_4  \\\n",
      "count      40.000000      40.000000      40.000000     40.000000   \n",
      "mean    58329.498565   50022.942854   30102.440637  20231.937041   \n",
      "std     48982.294031   51740.669944   31112.994651   5157.262333   \n",
      "min     15353.284864    2899.501846       0.000000  11736.569943   \n",
      "25%     20405.939998   14156.855130       0.000000  15186.656605   \n",
      "50%     28955.347830   25621.540289   23037.210884  21344.833969   \n",
      "75%     92678.659094   57755.626391   37781.857538  23876.932591   \n",
      "max    198178.600760  154148.240154  129931.737491  33292.977466   \n",
      "\n",
      "           Window_5       Window_6       Window_7       Window_8  \\\n",
      "count     40.000000      40.000000      40.000000      40.000000   \n",
      "mean   30722.989694   31697.989038   26777.577921   80388.236589   \n",
      "std    13544.809897   33585.123690   38661.661555   83627.400016   \n",
      "min     8807.718392    5402.465833       0.000000       0.000000   \n",
      "25%    18293.372363   10489.817800    8678.366611    7873.420226   \n",
      "50%    35173.125820   24993.605971   12926.432558   53166.234563   \n",
      "75%    39777.317525   34755.347295   35105.758953  107628.746580   \n",
      "max    54226.260781  167714.035965  207843.906848  295549.604403   \n",
      "\n",
      "           Window_9      Window_10     Window_11     Window_12      Window_13  \\\n",
      "count     40.000000      40.000000     40.000000     40.000000      40.000000   \n",
      "mean    9588.893925   57673.020899  46394.868852  21879.561441   42374.679275   \n",
      "std     7787.659516   73087.621817  25712.279991  12828.789494   38436.359243   \n",
      "min        0.000000       0.000000   6775.563126      0.000000   12617.781115   \n",
      "25%     4789.551396    8459.347484  28285.225583  14976.398779   20030.862048   \n",
      "50%     7502.372642   19625.095950  43772.836439  16496.941382   28870.166511   \n",
      "75%    13885.409793  116173.769032  72139.740068  30086.134027   48420.316466   \n",
      "max    30111.688620  214473.150605  86670.572193  56883.236081  181875.422465   \n",
      "\n",
      "           Window_14  \n",
      "count      40.000000  \n",
      "mean    22529.377522  \n",
      "std     50244.811577  \n",
      "min      4042.324020  \n",
      "25%      4675.100145  \n",
      "50%     10295.546996  \n",
      "75%     16073.948370  \n",
      "max    269642.514937  \n",
      "\n",
      "Count of NaN values in each column:\n",
      "Window_1         544\n",
      "Window_2         544\n",
      "Window_3         544\n",
      "Window_4         544\n",
      "Window_5         544\n",
      "Window_6         544\n",
      "Window_7         544\n",
      "Window_8         544\n",
      "Window_9         544\n",
      "Window_10        544\n",
      "Window_11        544\n",
      "Window_12        544\n",
      "Window_13        544\n",
      "Window_14        544\n",
      "emotion_class      9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the description of the DataFrame\n",
    "print(\"DataFrame Description:\")\n",
    "print(concatenated_df1.describe())\n",
    "\n",
    "# Display the count of NaN values in each column\n",
    "nan_values = concatenated_df1.isna().sum()\n",
    "print(\"\\nCount of NaN values in each column:\")\n",
    "print(nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f239fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_7156\\512803890.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  hvla_rows = Amigos_with_three_class[concatenated_df1['emotion_class'] == 'NaN '].index.tolist()\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hvla_rows \u001b[38;5;241m=\u001b[39m Amigos_with_three_class[concatenated_df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_class\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaN \u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Drop rows with HVLA and rows with the same index\u001b[39;00m\n\u001b[0;32m      4\u001b[0m concatenated_df1 \u001b[38;5;241m=\u001b[39m concatenated_df1\u001b[38;5;241m.\u001b[39mdrop(hvla_rows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3887\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3885\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_bool_array(key)\n\u001b[0;32m   3889\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3890\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3891\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3943\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3938\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItem wrong length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3939\u001b[0m     )\n\u001b[0;32m   3941\u001b[0m \u001b[38;5;66;03m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;66;03m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[1;32m-> 3943\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m   3945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m   3946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2575\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2573\u001b[0m indexer \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_indexer_for(index)\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m indexer:\n\u001b[1;32m-> 2575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\n\u001b[0;32m   2576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnalignable boolean Series provided as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindexer (index of the boolean Series and of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe indexed object do not match).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2579\u001b[0m     )\n\u001b[0;32m   2581\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;66;03m# fall through for boolean\u001b[39;00m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "hvla_rows = Amigos_with_three_class[concatenated_df1['emotion_class'] == 'NaN '].index.tolist()\n",
    "\n",
    "# Drop rows with HVLA and rows with the same index\n",
    "concatenated_df1 = concatenated_df1.drop(hvla_rows)\n",
    "concatenated_df1 = concatenated_df1.drop(hvla_rows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c1d25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvla_rows = Amigos_with_three_class.index[Amigos_with_three_class['emotion_class'].isna()].tolist()\n",
    "concatenated_df1 = concatenated_df1.drop(hvla_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99da974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Description:\n",
      "            Window_1       Window_2       Window_3      Window_4  \\\n",
      "count      40.000000      40.000000      40.000000     40.000000   \n",
      "mean    58329.498565   50022.942854   30102.440637  20231.937041   \n",
      "std     48982.294031   51740.669944   31112.994651   5157.262333   \n",
      "min     15353.284864    2899.501846       0.000000  11736.569943   \n",
      "25%     20405.939998   14156.855130       0.000000  15186.656605   \n",
      "50%     28955.347830   25621.540289   23037.210884  21344.833969   \n",
      "75%     92678.659094   57755.626391   37781.857538  23876.932591   \n",
      "max    198178.600760  154148.240154  129931.737491  33292.977466   \n",
      "\n",
      "           Window_5       Window_6       Window_7       Window_8  \\\n",
      "count     40.000000      40.000000      40.000000      40.000000   \n",
      "mean   30722.989694   31697.989038   26777.577921   80388.236589   \n",
      "std    13544.809897   33585.123690   38661.661555   83627.400016   \n",
      "min     8807.718392    5402.465833       0.000000       0.000000   \n",
      "25%    18293.372363   10489.817800    8678.366611    7873.420226   \n",
      "50%    35173.125820   24993.605971   12926.432558   53166.234563   \n",
      "75%    39777.317525   34755.347295   35105.758953  107628.746580   \n",
      "max    54226.260781  167714.035965  207843.906848  295549.604403   \n",
      "\n",
      "           Window_9      Window_10     Window_11     Window_12      Window_13  \\\n",
      "count     40.000000      40.000000     40.000000     40.000000      40.000000   \n",
      "mean    9588.893925   57673.020899  46394.868852  21879.561441   42374.679275   \n",
      "std     7787.659516   73087.621817  25712.279991  12828.789494   38436.359243   \n",
      "min        0.000000       0.000000   6775.563126      0.000000   12617.781115   \n",
      "25%     4789.551396    8459.347484  28285.225583  14976.398779   20030.862048   \n",
      "50%     7502.372642   19625.095950  43772.836439  16496.941382   28870.166511   \n",
      "75%    13885.409793  116173.769032  72139.740068  30086.134027   48420.316466   \n",
      "max    30111.688620  214473.150605  86670.572193  56883.236081  181875.422465   \n",
      "\n",
      "           Window_14  \n",
      "count      40.000000  \n",
      "mean    22529.377522  \n",
      "std     50244.811577  \n",
      "min      4042.324020  \n",
      "25%      4675.100145  \n",
      "50%     10295.546996  \n",
      "75%     16073.948370  \n",
      "max    269642.514937  \n",
      "\n",
      "Count of NaN values in each column:\n",
      "Window_1         544\n",
      "Window_2         544\n",
      "Window_3         544\n",
      "Window_4         544\n",
      "Window_5         544\n",
      "Window_6         544\n",
      "Window_7         544\n",
      "Window_8         544\n",
      "Window_9         544\n",
      "Window_10        544\n",
      "Window_11        544\n",
      "Window_12        544\n",
      "Window_13        544\n",
      "Window_14        544\n",
      "emotion_class      9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the description of the DataFrame\n",
    "print(\"DataFrame Description:\")\n",
    "print(concatenated_df1.describe())\n",
    "\n",
    "# Display the count of NaN values in each column\n",
    "nan_values = concatenated_df1.isna().sum()\n",
    "print(\"\\nCount of NaN values in each column:\")\n",
    "print(nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b145ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_7156\\3591162062.py:1: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df1['emotion_class'].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "concatenated_df1['emotion_class'].fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12449615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of NaN values in 'emotion_class' column: 9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nan_values_in_emotion_class = concatenated_df1['emotion_class'].isna().sum()\n",
    "\n",
    "print(\"Count of NaN values in 'emotion_class' column:\", nan_values_in_emotion_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab3c0f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window_1</th>\n",
       "      <th>Window_2</th>\n",
       "      <th>Window_3</th>\n",
       "      <th>Window_4</th>\n",
       "      <th>Window_5</th>\n",
       "      <th>Window_6</th>\n",
       "      <th>Window_7</th>\n",
       "      <th>Window_8</th>\n",
       "      <th>Window_9</th>\n",
       "      <th>Window_10</th>\n",
       "      <th>Window_11</th>\n",
       "      <th>Window_12</th>\n",
       "      <th>Window_13</th>\n",
       "      <th>Window_14</th>\n",
       "      <th>emotion_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22080.029612</td>\n",
       "      <td>21307.532478</td>\n",
       "      <td>36061.299155</td>\n",
       "      <td>23897.913856</td>\n",
       "      <td>19294.550337</td>\n",
       "      <td>8494.271861</td>\n",
       "      <td>24999.216663</td>\n",
       "      <td>38887.260665</td>\n",
       "      <td>7459.637819</td>\n",
       "      <td>22778.943814</td>\n",
       "      <td>8530.692901</td>\n",
       "      <td>24114.632085</td>\n",
       "      <td>15714.249467</td>\n",
       "      <td>17938.503872</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24385.023872</td>\n",
       "      <td>19767.081042</td>\n",
       "      <td>33736.607923</td>\n",
       "      <td>25830.316767</td>\n",
       "      <td>12968.275356</td>\n",
       "      <td>9184.462114</td>\n",
       "      <td>40127.224066</td>\n",
       "      <td>31540.475070</td>\n",
       "      <td>9902.524014</td>\n",
       "      <td>21592.271382</td>\n",
       "      <td>8519.599590</td>\n",
       "      <td>24746.385405</td>\n",
       "      <td>16674.898451</td>\n",
       "      <td>14248.882606</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21047.319425</td>\n",
       "      <td>18749.789104</td>\n",
       "      <td>22910.775650</td>\n",
       "      <td>19535.653774</td>\n",
       "      <td>16263.368808</td>\n",
       "      <td>10517.512783</td>\n",
       "      <td>18884.003076</td>\n",
       "      <td>36929.067712</td>\n",
       "      <td>12428.933241</td>\n",
       "      <td>21687.209437</td>\n",
       "      <td>8226.798209</td>\n",
       "      <td>28837.641967</td>\n",
       "      <td>12617.781115</td>\n",
       "      <td>21695.196470</td>\n",
       "      <td>LALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15939.235580</td>\n",
       "      <td>8569.148604</td>\n",
       "      <td>26727.811820</td>\n",
       "      <td>23612.937825</td>\n",
       "      <td>10055.447586</td>\n",
       "      <td>11337.317740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32859.095530</td>\n",
       "      <td>11303.738230</td>\n",
       "      <td>26720.034562</td>\n",
       "      <td>7038.752072</td>\n",
       "      <td>38700.847863</td>\n",
       "      <td>15589.878058</td>\n",
       "      <td>15122.099687</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27571.004599</td>\n",
       "      <td>2899.501846</td>\n",
       "      <td>22917.107839</td>\n",
       "      <td>24866.900934</td>\n",
       "      <td>11843.631573</td>\n",
       "      <td>7446.844936</td>\n",
       "      <td>18143.293540</td>\n",
       "      <td>35745.017041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24256.992266</td>\n",
       "      <td>6775.563126</td>\n",
       "      <td>29983.747214</td>\n",
       "      <td>15039.120230</td>\n",
       "      <td>26636.190642</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Window_1      Window_2      Window_3      Window_4      Window_5  \\\n",
       "0  22080.029612  21307.532478  36061.299155  23897.913856  19294.550337   \n",
       "1  24385.023872  19767.081042  33736.607923  25830.316767  12968.275356   \n",
       "2  21047.319425  18749.789104  22910.775650  19535.653774  16263.368808   \n",
       "3  15939.235580   8569.148604  26727.811820  23612.937825  10055.447586   \n",
       "4  27571.004599   2899.501846  22917.107839  24866.900934  11843.631573   \n",
       "\n",
       "       Window_6      Window_7      Window_8      Window_9     Window_10  \\\n",
       "0   8494.271861  24999.216663  38887.260665   7459.637819  22778.943814   \n",
       "1   9184.462114  40127.224066  31540.475070   9902.524014  21592.271382   \n",
       "2  10517.512783  18884.003076  36929.067712  12428.933241  21687.209437   \n",
       "3  11337.317740      0.000000  32859.095530  11303.738230  26720.034562   \n",
       "4   7446.844936  18143.293540  35745.017041      0.000000  24256.992266   \n",
       "\n",
       "     Window_11     Window_12     Window_13     Window_14 emotion_class  \n",
       "0  8530.692901  24114.632085  15714.249467  17938.503872          LALV  \n",
       "1  8519.599590  24746.385405  16674.898451  14248.882606          LAHV  \n",
       "2  8226.798209  28837.641967  12617.781115  21695.196470          LALV  \n",
       "3  7038.752072  38700.847863  15589.878058  15122.099687          LAHV  \n",
       "4  6775.563126  29983.747214  15039.120230  26636.190642          LAHV  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06eadc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 41.27%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df1.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df1['emotion_class']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5]\n",
    "}\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y_encoded)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X, y_encoded, sample_weight=np.array([class_weights[i] for i in y_encoded]))\n",
    "\n",
    "# Make predictions oen the test set\n",
    "best_predictions = best_model.predict(X)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_text, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbce98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X = concatenated_df1.drop('emotion_class', axis=1)\n",
    "y_text = concatenated_df1['emotion_class']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_text)\n",
    "\n",
    "# Calculate class weights to handle imbalanced classes\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 2]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Build the XGBoost model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=np.max(y_encoded) + 1, random_state=42)\n",
    "best_model.fit(X_train, y_train, sample_weight=np.array([class_weights[i] for i in y_train]))\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "best_predicted_labels = label_encoder.inverse_transform(best_predictions)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "best_accuracy = accuracy_score(y_test, best_predicted_labels)\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb32bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
